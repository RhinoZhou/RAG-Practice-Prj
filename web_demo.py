# -*- coding: utf-8 -*-
'''
æ³•å¾‹æ™ºèƒ½é—®ç­”ç³»ç»ŸWebæ¼”ç¤ºç•Œé¢ï¼ŒåŸºäºStreamlitæ„å»ºã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. ç”¨æˆ·å‹å¥½çš„å¯¹è¯ç•Œé¢ï¼Œæ”¯æŒå¤šè½®å¯¹è¯äº¤äº’
2. é›†æˆæ³•å¾‹é—®é¢˜åˆ†ç±»å™¨ï¼Œè‡ªåŠ¨è¯†åˆ«é—®é¢˜ç±»å‹
3. åŸºäºRAGæŠ€æœ¯ï¼Œä»æ³•å¾‹çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
4. è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå‡†ç¡®çš„æ³•å¾‹å›ç­”
5. å±•ç¤ºå›ç­”çš„å‚è€ƒèµ„æ–™æ¥æº
è¯¥æ¼”ç¤ºç•Œé¢å±•ç¤ºäº†å®Œæ•´çš„æ³•å¾‹RAGç³»ç»Ÿå·¥ä½œæµç¨‹ã€‚
'''

import json
import torch
import streamlit as st
import os
import sys
from typing import Tuple, List, Dict, Any

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from RAG import *
from Questionary import QuestionClassifier


def setup_dependencies():
    """
    æ£€æŸ¥å¹¶è‡ªåŠ¨å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…
    """
    required_packages = [
        'streamlit',
        'torch',
        'transformers',
        'sentence-transformers',
        'pymilvus'
    ]
    
    print("æ­£åœ¨æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–åŒ…...")
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ“ {package} å·²å®‰è£…")
        except ImportError:
            print(f"âœ— {package} æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            try:
                os.system(f"pip install {package}")
                print(f"âœ“ {package} å®‰è£…æˆåŠŸ")
            except Exception as e:
                print(f"âœ— {package} å®‰è£…å¤±è´¥: {str(e)}")


# è‡ªåŠ¨å®‰è£…ä¾èµ–
setup_dependencies()


# é…ç½®é¡µé¢
st.set_page_config(
    page_title="æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ - LawBot",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="auto"
)

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œä»‹ç»
st.title("æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ ğŸ¤–")
st.markdown("ä¸“ä¸šçš„æ³•å¾‹é—®ç­”ç³»ç»Ÿï¼ŒåŸºäºRAGæŠ€æœ¯æä¾›å‡†ç¡®çš„æ³•å¾‹ä¿¡æ¯å’Œå»ºè®®")


@st.cache_resource
def init_model() -> Tuple[Any, Any, Any, Any, QuestionClassifier]:
    """
    åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶ï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åŠ è½½
    
    Returns:
        Tuple: åŒ…å«LLMã€åµŒå…¥æ¨¡å‹ã€å‘é‡æ•°æ®åº“ã€é‡æ’åºæ¨¡å‹å’Œåˆ†ç±»å™¨çš„å…ƒç»„
    """
    try:
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œæé«˜ä»£ç å¯ç§»æ¤æ€§
        base_model_dir = './model_hub' if os.path.exists('./model_hub') else './models'
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹ç»„ä»¶...")
        
        # åŠ è½½å¤§è¯­è¨€æ¨¡å‹
        print("åŠ è½½å¤§è¯­è¨€æ¨¡å‹...")
        try:
            # å°è¯•ä½¿ç”¨åŸå§‹è·¯å¾„ï¼ŒåŒæ—¶æä¾›ç›¸å¯¹è·¯å¾„é€‰é¡¹
            llm_paths = [
                '/root/sunyd/model_hub/qwen/Qwen2-7B-Instruct',
                os.path.join(base_model_dir, 'qwen/Qwen2-7B-Instruct'),
                os.path.join(base_model_dir, 'qwen')
            ]
            
            for path in llm_paths:
                try:
                    if os.path.exists(path):
                        llm = QwenModelChat(path)
                        print(f"æˆåŠŸåŠ è½½LLMæ¨¡å‹: {path}")
                        break
                except Exception:
                    continue
            else:
                # å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç®€å•å®ç°
                print("æ— æ³•æ‰¾åˆ°LLMæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                llm = QwenModelChat()  # å‡è®¾æ„é€ å‡½æ•°æœ‰é»˜è®¤å‚æ•°
                
        except Exception as e:
            print(f"LLMæ¨¡å‹åŠ è½½è­¦å‘Š: {str(e)}")
            # ç»§ç»­æ‰§è¡Œï¼Œå¸Œæœ›èƒ½å¤Ÿä½¿ç”¨é»˜è®¤æ„é€ å‡½æ•°
            llm = QwenModelChat()
        
        # åŠ è½½åµŒå…¥æ¨¡å‹
        print("åŠ è½½åµŒå…¥æ¨¡å‹...")
        try:
            embedding_paths = [
                '/root/sunyd/model_hub/ZhipuAI/bge-large-zh-v1___5/',
                os.path.join(base_model_dir, 'ZhipuAI/bge-large-zh-v1.5'),
                os.path.join(base_model_dir, 'bge-large-zh')
            ]
            
            for path in embedding_paths:
                try:
                    if os.path.exists(path):
                        embedding = BGEVectorizer(path)
                        print(f"æˆåŠŸåŠ è½½åµŒå…¥æ¨¡å‹: {path}")
                        break
                except Exception:
                    continue
            else:
                embedding = BGEVectorizer()
                
        except Exception as e:
            print(f"åµŒå…¥æ¨¡å‹åŠ è½½è­¦å‘Š: {str(e)}")
            embedding = BGEVectorizer()
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        print("åŠ è½½å‘é‡æ•°æ®åº“...")
        try:
            vector_db_paths = [
                '/root/sunyd/llms/TinyRAG-master/storage/milvus_law.db',
                './storage/milvus_law.db',
                './data/milvus_law.db'
            ]
            
            for path in vector_db_paths:
                try:
                    if os.path.exists(os.path.dirname(path)) or os.path.exists(path):
                        vector = VectorStore(uri=path)
                        print(f"æˆåŠŸåŠ è½½å‘é‡æ•°æ®åº“: {path}")
                        break
                except Exception:
                    continue
            else:
                vector = VectorStore(uri='./milvus_law.db')
                
        except Exception as e:
            print(f"å‘é‡æ•°æ®åº“åŠ è½½è­¦å‘Š: {str(e)}")
            vector = VectorStore(uri='./milvus_law.db')
        
        # åŠ è½½é‡æ’åºæ¨¡å‹
        print("åŠ è½½é‡æ’åºæ¨¡å‹...")
        try:
            reranker_paths = [
                '/root/sunyd/model_hub/Xorbits/bge-reranker-base',
                os.path.join(base_model_dir, 'Xorbits/bge-reranker-base'),
                os.path.join(base_model_dir, 'bge-reranker-base')
            ]
            
            for path in reranker_paths:
                try:
                    if os.path.exists(path):
                        relevance_reranker = BgeReranker(path=path)
                        print(f"æˆåŠŸåŠ è½½é‡æ’åºæ¨¡å‹: {path}")
                        break
                except Exception:
                    continue
            else:
                relevance_relevance_reranker = BgeReranker()
                
        except Exception as e:
            print(f"é‡æ’åºæ¨¡å‹åŠ è½½è­¦å‘Š: {str(e)}")
            reranker = BgeReranker()
        
        # åˆå§‹åŒ–é—®é¢˜åˆ†ç±»å™¨
        print("åˆå§‹åŒ–é—®é¢˜åˆ†ç±»å™¨...")
        classifier = QuestionClassifier()
        
        print("æ‰€æœ‰æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆï¼")
        return llm, embedding, vector, relevance_reranker, classifier
    
    except Exception as e:
        print(f"æ¨¡å‹åˆå§‹åŒ–é”™è¯¯: {str(e)}")
        st.error(f"ç³»ç»Ÿåˆå§‹åŒ–é”™è¯¯: {str(e)}")
        # å°è¯•è¿”å›å¯ç”¨çš„ç»„ä»¶
        try:
            return llm, embedding, vector, relevance_reranker, QuestionClassifier()
        except:
            return None, None, None, None, None


def clear_chat_history():
    """
    æ¸…ç©ºå¯¹è¯å†å²è®°å½•
    """
    if "messages" in st.session_state:
        del st.session_state.messages
        st.success("å¯¹è¯å†å²å·²æ¸…ç©º")


def init_chat_history() -> List[Dict[str, str]]:
    """
    åˆå§‹åŒ–å¯¹è¯å†å²ï¼Œæ˜¾ç¤ºæ¬¢è¿æ¶ˆæ¯å’Œä¹‹å‰çš„å¯¹è¯å†…å®¹
    
    Returns:
        List[Dict]: å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨
    """
    # æ˜¾ç¤ºæ¬¢è¿æ¶ˆæ¯
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown("æ‚¨å¥½ï¼æˆ‘æ˜¯æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æä¾›æ³•å¾‹å’¨è¯¢æœåŠ¡ã€‚è¯·è¾“å…¥æ‚¨çš„æ³•å¾‹é—®é¢˜ï¼Œæˆ‘å°†å°½åŠ›ä¸ºæ‚¨è§£ç­”ã€‚")

    # æ˜¾ç¤ºä¹‹å‰çš„å¯¹è¯æ¶ˆæ¯
    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = "ğŸ™‹â€â™‚ï¸" if message["role"] == "user" else "ğŸ¤–"
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        # åˆå§‹åŒ–ç©ºçš„å¯¹è¯å†å²
        st.session_state.messages = []

    return st.session_state.messages


def process_query(question: str, llm: Any, embedding: Any, vector: Any, 
                 relevance_reranker: Any, classifier: QuestionClassifier) -> str:
    """
    å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ŒåŒ…æ‹¬é—®é¢˜åˆ†ç±»ã€çŸ¥è¯†æ£€ç´¢å’Œå›ç­”ç”Ÿæˆ
    
    Args:
        question (str): ç”¨æˆ·é—®é¢˜
        llm: å¤§è¯­è¨€æ¨¡å‹å®ä¾‹
        embedding: åµŒå…¥æ¨¡å‹å®ä¾‹
        vector: å‘é‡æ•°æ®åº“å®ä¾‹
        reranker: é‡æ’åºæ¨¡å‹å®ä¾‹
        classifier: é—®é¢˜åˆ†ç±»å™¨å®ä¾‹
        
    Returns:
        str: ç”Ÿæˆçš„å›ç­”
    """
    try:
        # å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»
        res_classify = classifier.classify(question)
        
        # æ£€æŸ¥åˆ†ç±»ç»“æœ
        if len(res_classify['kg_names']) == 0:
            # æœªè¯†åˆ«åˆ°ç‰¹å®šç±»åˆ«ï¼Œç›´æ¥ä½¿ç”¨LLMå›ç­”
            print(f"æœªè¯†åˆ«åˆ°ç‰¹å®šç±»åˆ«ï¼Œç›´æ¥ä½¿ç”¨LLMå›ç­”é—®é¢˜: {question}")
            prompt = llm.generate_prompt(question, "")
            answer = llm.chat(prompt)
            return answer
        else:
            print(f"è¯†åˆ«åˆ°é—®é¢˜ç±»åˆ«: {res_classify['kg_names']}")
            
            # ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³å†…å®¹
            contents = []
            sim_query = []
            
            for collection_name in res_classify['kg_names']:
                print(f"ä»é›†åˆ {collection_name} ä¸­æ£€ç´¢ç›¸å…³å†…å®¹...")
                try:
                    # æ£€ç´¢å‰kä¸ªæœ€ç›¸å…³çš„å†…å®¹
                    for content in vector.query(question, collection_name=collection_name, 
                                             vectorizer=embedding, k=3):
                        sim_query.append(content.key)
                        contents.append(content.value)
                except Exception as e:
                    print(f"æ£€ç´¢é›†åˆ {collection_name} æ—¶å‡ºé”™: {str(e)}")
            
            # æ£€æŸ¥æ˜¯å¦æ£€ç´¢åˆ°å†…å®¹
            if len(contents) == 0:
                # æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨LLMå›ç­”
                print("æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨LLMå›ç­”")
                prompt = llm.generate_prompt(question, "")
                answer = llm.chat(prompt)
                return answer
            else:
                # æ„å»ºå‚è€ƒèµ„æ–™æ–‡æœ¬
                best_content = "å‚è€ƒèµ„æ–™ï¼š"
                for i, sq in enumerate(contents, 1):
                    best_content += f'\n\n{i}. {sq}'
                
                # æ˜¾ç¤ºå‚è€ƒèµ„æ–™
                with st.chat_message("assistant", avatar="ğŸ¤–"):
                    st.markdown("**å‚è€ƒèµ„æ–™ï¼š**")
                    for i, sq in enumerate(contents, 1):
                        st.markdown(f"{i}. {sq}")
                
                # ä½¿ç”¨æ£€ç´¢åˆ°çš„å†…å®¹æ„å»ºæç¤ºå¹¶ç”Ÿæˆå›ç­”
                print(f"ä½¿ç”¨æ£€ç´¢åˆ°çš„{len(contents)}æ¡å†…å®¹ç”Ÿæˆå›ç­”")
                prompt = llm.generate_prompt(question, best_content)
                answer = llm.chat(prompt)
                return answer
    
    except Exception as e:
        print(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
        return f"å¾ˆæŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"


def main():
    """
    ä¸»å‡½æ•°ï¼Œè¿è¡ŒStreamlitåº”ç”¨
    """
    # åˆå§‹å›ç­”æ–‡æœ¬
    default_answer = 'æ‚¨å¥½ï¼æˆ‘æ˜¯æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆéœ€è¦å’¨è¯¢çš„æ³•å¾‹é—®é¢˜ï¼Ÿ'
    
    # åŠ è½½æ¨¡å‹ç»„ä»¶
    llm, embedding, vector, relevance_reranker, classifier = init_model()
    
    # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯å¹¶é€€å‡º
    if llm is None:
        st.error("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„å’Œä¾èµ–å®‰è£…æƒ…å†µã€‚")
        return
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    messages = init_chat_history()
    
    # æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜æç¤º
    with st.expander("ğŸ’¡ ç¤ºä¾‹é—®é¢˜", expanded=False):
        st.markdown("""
        - ã€Šæ°‘æ³•å…¸ã€‹ä¸­å…³äºåˆåŒè¿çº¦çš„è§„å®šæœ‰å“ªäº›ï¼Ÿ
        - å¦‚ä½•æ’°å†™ä¸€ä»½æœ‰æ•ˆçš„é—å˜±ï¼Ÿ
        - å¼ ä¸‰å› ç›—çªƒç½ªè¢«èµ·è¯‰ï¼Œå¯èƒ½ä¼šé¢ä¸´ä»€ä¹ˆå¤„ç½šï¼Ÿ
        - æ¨èå‡ æœ¬å…³äºå•†æ³•çš„ç»å…¸è‘—ä½œ
        - æ³•è€ƒé¢˜ç›®ï¼šä»¥ä¸‹å…³äºæ­£å½“é˜²å«çš„è¯´æ³•æ­£ç¡®çš„æ˜¯ï¼Ÿ
        """)
    
    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨çš„æ³•å¾‹é—®é¢˜ï¼ŒæŒ‰Enterå‘é€"):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user", avatar="ğŸ™‹â€â™‚ï¸"):
            st.markdown(question)
        
        # å¤„ç†æŸ¥è¯¢å¹¶ç”Ÿæˆå›ç­”
        answer = process_query(question, llm, embedding, vector, relevance_reranker, classifier)
        
        # æ·»åŠ åˆ°å¯¹è¯å†å²
        messages.append({"role": "user", "content": question})
        
        # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            placeholder = st.empty()
            placeholder.markdown(answer)
            
            # æ¸…ç†ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
        
        # æ·»åŠ å›ç­”åˆ°å¯¹è¯å†å²
        messages.append({"role": "assistant", "content": answer})
        
        # æ‰“å°å¯¹è¯å†å²åˆ°æ§åˆ¶å°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print(json.dumps(messages, ensure_ascii=False), flush=True)
    
    # æ¸…ç©ºå¯¹è¯æŒ‰é’®
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        st.button("ğŸ”„ æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    # ç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
    st.markdown("<style>body { font-family: 'SimHei', 'WenQuanYi Micro Hei', sans-serif; }</style>", 
                unsafe_allow_html=True)
    
    # è¿è¡Œä¸»å‡½æ•°
    main()