#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸€è‡´æ€§é˜ˆå€¼å®ˆé—¨ä¸å›é€€

ä½œè€…: Ph.D. Rhino
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¥æœŸ: 2024-01-19

åŠŸèƒ½è¯´æ˜:
è®¡ç®—æ‘˜è¦ä¸ç»†èŠ‚ä¸€è‡´æ€§ï¼Œä½äºé˜ˆå€¼å›é€€åˆ°å…¨æ–‡é‡æ’ã€‚

å†…å®¹æ¦‚è¿°:
åœ¨åŒåº“æ£€ç´¢åŸºç¡€ä¸Šï¼Œè®¡ç®—"æ‘˜è¦å¥ â†” ç»†èŠ‚æ®µ"ä¸€è‡´æ€§å‡åˆ†ï¼›è‹¥ä½äºé˜ˆå€¼Ï„ï¼Œè§¦å‘å›é€€ï¼Œå¯¹å…¨åº“æ®µè½é‡æ’ä¿éšœå¬å›ç¨³å®šï¼Œè¾“å‡ºä¸€è‡´æ€§åˆ†ã€å›é€€æ ‡å¿—ä¸æœ€ç»ˆå€™é€‰ã€‚

ä½¿ç”¨åœºæ™¯:
- RAGç³»ç»Ÿä¸­çš„æ£€ç´¢è´¨é‡ä¿éšœ
- é˜²æ­¢å› æ‘˜è¦è¯¯åŒ¹é…å¯¼è‡´çš„ç»†èŠ‚æ£€ç´¢åå·®
- é€šè¿‡å›é€€æœºåˆ¶æé«˜æ£€ç´¢çš„ç¨³å¥æ€§

ä¾èµ–åº“:
- sklearn: ç”¨äºTF-IDFå‘é‡åŒ–å’Œç›¸ä¼¼åº¦è®¡ç®—
- jieba: ç”¨äºä¸­æ–‡åˆ†è¯
- numpy: ç”¨äºæ•°å€¼è®¡ç®—
"""

# è‡ªåŠ¨å®‰è£…ä¾èµ–åº“
import subprocess
import sys
import time

# å®šä¹‰æ‰€éœ€ä¾èµ–åº“
required_dependencies = [
    'scikit-learn',
    'jieba',
    'numpy'
]


def install_dependencies():
    """æ£€æŸ¥å¹¶è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–åº“"""
    for dependency in required_dependencies:
        try:
            # å°è¯•å¯¼å…¥åº“ä»¥æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
            __import__(dependency.replace('-', '_'))  # å¤„ç†scikit-learnçš„å‘½åå·®å¼‚
            print(f"âœ… ä¾èµ–åº“ '{dependency}' å·²å®‰è£…")
        except ImportError:
            print(f"âš ï¸ ä¾èµ–åº“ '{dependency}' æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            # ä½¿ç”¨pipå®‰è£…ç¼ºå¤±çš„ä¾èµ–
            subprocess.check_call([sys.executable, "-m", "pip", "install", dependency])
            print(f"âœ… ä¾èµ–åº“ '{dependency}' å®‰è£…æˆåŠŸ")


# æ‰§è¡Œä¾èµ–å®‰è£…
if __name__ == "__main__":
    install_dependencies()


# å¯¼å…¥æ‰€éœ€åº“
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import numpy as np
import json


class ConsistencyGateAndFallback:
    """ä¸€è‡´æ€§é˜ˆå€¼å®ˆé—¨ä¸å›é€€å®ç°ç±»"""
    
    # é…ç½®å‚æ•° - å¯åœ¨é¡¶éƒ¨è°ƒæ•´
    TAU = 0.85  # æé«˜é˜ˆå€¼ä»¥æ›´å®¹æ˜“è§¦å‘å›é€€æ¼”ç¤º
    TOP_K_SECTIONS = 1  # å¬å›çš„ç« èŠ‚æ•°é‡
    TOP_K_DETAILS = 3   # æ¯ä¸ªç« èŠ‚å¬å›çš„ç»†èŠ‚æ®µè½æ•°é‡
    TOP_K_FINAL = 5     # æœ€ç»ˆè¾“å‡ºçš„æ®µè½æ•°é‡
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸€è‡´æ€§é˜ˆå€¼å®ˆé—¨ä¸å›é€€ç³»ç»Ÿ"""
        # åˆå§‹åŒ–æ•°æ®ç»“æ„
        self.summary_corpus = []  # æ‘˜è¦å¥åº“
        self.detail_corpus = {}   # ç»†èŠ‚æ®µè½åº“ {section_id: [paragraphs]}
        self.global_paragraphs = []  # å…¨å±€æ®µè½åº“ï¼Œç”¨äºå›é€€æ—¶çš„å…¨æ–‡é‡æ’
        self.global_paragraph_info = []  # å…¨å±€æ®µè½ä¿¡æ¯
        self.section_info = []    # ç« èŠ‚ä¿¡æ¯ [(doc_id, section_id, anchor, summary_text)]
        self.vectorizer = None    # TF-IDFå‘é‡åŒ–å™¨
        self.summary_matrix = None  # æ‘˜è¦å‘é‡çŸ©é˜µ
        self.global_matrix = None   # å…¨å±€æ®µè½å‘é‡çŸ©é˜µ
        
        # åŠ è½½ç¤ºä¾‹æ•°æ®
        self._load_sample_data()
        # åˆå§‹åŒ–TF-IDFæ¨¡å‹
        self._initialize_tfidf()
    
    def _load_sample_data(self):
        """åŠ è½½ç¤ºä¾‹æ•°æ®ï¼šæ„å»ºæ‘˜è¦å¥åº“ã€ç»†èŠ‚æ®µè½åº“å’Œå…¨å±€æ®µè½åº“"""
        # ç¤ºä¾‹æ–‡æ¡£æ•°æ® - æŠ€æœ¯ç™½çš®ä¹¦çš„éƒ¨åˆ†ç« èŠ‚
        self.section_info = [
            # (æ–‡æ¡£ID, ç« èŠ‚ID, é”šç‚¹, æ‘˜è¦æ–‡æœ¬)
            ("docA", "1.1", "anc:sec-1-1", "RAGç³»ç»Ÿæ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„æ ¸å¿ƒå®ç°æ–¹æ¡ˆã€‚"),
            ("docA", "1.2", "anc:sec-1-2", "æ£€ç´¢æ¨¡å—è´Ÿè´£ä»å¤–éƒ¨çŸ¥è¯†åº“è·å–ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚"),
            ("docA", "2.1", "anc:sec-2-1", "å‘é‡æ£€ç´¢æ˜¯åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ£€ç´¢æ–¹æ³•ã€‚"),
            ("docA", "2.2", "anc:sec-2-2", "æ··åˆæ£€ç´¢ç»“åˆå…³é”®è¯åŒ¹é…ä¸è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚"),
            ("docA", "3.1", "anc:sec-3-1", "æ–‡æœ¬åˆ†å—æ˜¯RAGç³»ç»Ÿä¸­çš„é‡è¦é¢„å¤„ç†æ­¥éª¤ã€‚"),
            ("docA", "3.2", "anc:sec-3-2", "çª—å£æ»‘åŠ¨æ˜¯å¸¸ç”¨çš„æ–‡æœ¬åˆ†å—ç­–ç•¥ä¹‹ä¸€ã€‚"),
        ]
        
        # æ„å»ºæ‘˜è¦å¥åº“
        for _, _, _, summary in self.section_info:
            self.summary_corpus.append(summary)
        
        # æ„å»ºç»†èŠ‚æ®µè½åº“å’Œå…¨å±€æ®µè½åº“
        self.detail_corpus = {
            "1.1": [  # ç« èŠ‚1.1çš„ç»†èŠ‚æ®µè½
                {"id": "p1", "anchor": "anc:p1-1-1", "content": "RAGç³»ç»Ÿé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯ä¸ç”¨æˆ·æŸ¥è¯¢ä¸€èµ·è¾“å…¥åˆ°ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œç”Ÿæˆæ›´åŠ å‡†ç¡®ã€å…¨é¢çš„å›ç­”ã€‚"},
                {"id": "p2", "anchor": "anc:p2-1-1", "content": "æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æœ‰æ•ˆåœ°å¼¥è¡¥äº†å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œå®æ—¶ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ã€‚"},
                {"id": "p3", "anchor": "anc:p3-1-1", "content": "RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬æ£€ç´¢æ¨¡å—ã€çŸ¥è¯†åº“å’Œç”Ÿæˆæ¨¡å—ã€‚"}
            ],
            "1.2": [  # ç« èŠ‚1.2çš„ç»†èŠ‚æ®µè½
                {"id": "p1", "anchor": "anc:p1-1-2", "content": "æ£€ç´¢æ¨¡å—æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œè¿™äº›ç‰‡æ®µåŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„äº‹å®ä¿¡æ¯ã€‚"},
                {"id": "p2", "anchor": "anc:p2-1-2", "content": "æ£€ç´¢çš„å‡†ç¡®æ€§ç›´æ¥å½±å“ç”Ÿæˆå›ç­”çš„è´¨é‡ï¼Œå› æ­¤æ£€ç´¢ç­–ç•¥çš„é€‰æ‹©è‡³å…³é‡è¦ã€‚"}
            ],
            "2.1": [  # ç« èŠ‚2.1çš„ç»†èŠ‚æ®µè½
                {"id": "p1", "anchor": "anc:p1-2-1", "content": "å‘é‡æ£€ç´¢é€šè¿‡å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡è¡¨ç¤ºï¼Œåˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ç­‰åº¦é‡è®¡ç®—æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚"},
                {"id": "p2", "anchor": "anc:p2-2-1", "content": "å¸¸ç”¨çš„å‘é‡è¡¨ç¤ºæ–¹æ³•åŒ…æ‹¬Word2Vecã€GloVeå’ŒåŸºäºTransformerçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚"}
            ],
            "2.2": [  # ç« èŠ‚2.2çš„ç»†èŠ‚æ®µè½
                {"id": "p1", "anchor": "anc:p1-2-2", "content": "æ··åˆæ£€ç´¢ç­–ç•¥ç»“åˆäº†å…³é”®è¯æ£€ç´¢çš„ç²¾ç¡®æ€§å’Œå‘é‡æ£€ç´¢çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•è·ç›¸å…³ä¿¡æ¯ã€‚"},
                {"id": "p2", "anchor": "anc:p2-2-2", "content": "å…³é”®è¯åŒ¹é…é€šè¿‡ç²¾ç¡®çš„æ–‡æœ¬åŒ¹é…å¿«é€Ÿå®šä½ç›¸å…³æ–‡æ¡£ï¼Œè€Œè¯­ä¹‰ç›¸ä¼¼æ€§è®¡ç®—åˆ™æ•è·æ·±å±‚å«ä¹‰çš„å…³è”ã€‚"},
                {"id": "p3", "anchor": "anc:p3-2-2", "content": "TF-IDFæ˜¯ä¸€ç§å¸¸ç”¨çš„å…³é”®è¯æ£€ç´¢æ–¹æ³•ï¼Œå®ƒæ ¹æ®è¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡è®¡ç®—è¯çš„é‡è¦æ€§ã€‚"},
                {"id": "p4", "anchor": "anc:p4-2-2", "content": "æ··åˆæ£€ç´¢ç³»ç»Ÿé€šå¸¸ä¼šç»“åˆå¤šç§æ£€ç´¢ç»“æœçš„åˆ†æ•°ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ’åºç»“æœã€‚"}
            ],
            "3.1": [  # ç« èŠ‚3.1çš„ç»†èŠ‚æ®µè½
                {"id": "p1", "anchor": "anc:p1-3-1", "content": "æ–‡æœ¬åˆ†å—æ˜¯å°†é•¿æ–‡æ¡£åˆ†å‰²æˆæ›´å°ã€æ›´æ˜“äºå¤„ç†çš„ç‰‡æ®µçš„è¿‡ç¨‹ï¼Œè¿™æœ‰åŠ©äºæé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œç”Ÿæˆçš„è´¨é‡ã€‚"},
                {"id": "p2", "anchor": "anc:p2-3-1", "content": "åˆé€‚çš„åˆ†å—å¤§å°éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ–‡æ¡£ç‰¹ç‚¹è¿›è¡Œè°ƒæ•´ï¼Œè¿‡å¤§æˆ–è¿‡å°çš„å—éƒ½å¯èƒ½å½±å“æ€§èƒ½ã€‚"}
            ],
            "3.2": [  # ç« èŠ‚3.2çš„ç»†èŠ‚æ®µè½
                {"id": "p1", "anchor": "anc:p1-3-2", "content": "çª—å£æ»‘åŠ¨ç­–ç•¥é€šè¿‡å›ºå®šå¤§å°çš„çª—å£å’Œé‡å éƒ¨åˆ†ï¼Œå°†æ–‡æœ¬åˆ’åˆ†ä¸ºè¿ç»­çš„å—ï¼Œä¿æŒäº†æ–‡æœ¬çš„å±€éƒ¨è¿è´¯æ€§ã€‚"},
                {"id": "p2", "anchor": "anc:p2-3-2", "content": "çª—å£å¤§å°å’Œé‡å æ¯”ä¾‹æ˜¯å½±å“åˆ†å—æ•ˆæœçš„ä¸¤ä¸ªå…³é”®å‚æ•°ï¼Œéœ€è¦æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚"}
            ]
        }
        
        # æ„å»ºå…¨å±€æ®µè½åº“å’Œå¯¹åº”çš„ä¿¡æ¯
        for section_id, paragraphs in self.detail_corpus.items():
            for para in paragraphs:
                # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£ID
                doc_id = "docA"  # ç¤ºä¾‹ä¸­æ‰€æœ‰æ®µè½éƒ½æ¥è‡ªåŒä¸€æ–‡æ¡£
                
                # æ·»åŠ åˆ°å…¨å±€æ®µè½åº“
                self.global_paragraphs.append(para['content'])
                self.global_paragraph_info.append({
                    'doc_id': doc_id,
                    'section_id': section_id,
                    'paragraph_id': para['id'],
                    'anchor': para['anchor'],
                    'content': para['content']
                })
    
    def _tokenize_chinese(self, text):
        """ä¸­æ–‡åˆ†è¯å¤„ç†"""
        # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        return ' '.join(jieba.cut(text))
    
    def _initialize_tfidf(self):
        """åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨"""
        # åˆ›å»ºä¸­æ–‡TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            tokenizer=self._tokenize_chinese,
            analyzer='word',
            lowercase=False,  # ä¸­æ–‡ä¸åŒºåˆ†å¤§å°å†™
            max_features=5000
        )
        
        # å¯¹æ‘˜è¦å¥åº“è¿›è¡Œå‘é‡åŒ–
        self.summary_matrix = self.vectorizer.fit_transform(self.summary_corpus)
        
        # å¯¹å…¨å±€æ®µè½åº“è¿›è¡Œå‘é‡åŒ–
        self.global_matrix = self.vectorizer.transform(self.global_paragraphs)
    
    def _search_summary(self, query):
        """åœ¨æ‘˜è¦åº“ä¸­æ£€ç´¢ç›¸å…³ç« èŠ‚"""
        # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
        query_vec = self.vectorizer.transform([query])
        
        # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰æ‘˜è¦çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vec, self.summary_matrix).flatten()
        
        # è·å–æ’åºåçš„ç´¢å¼•
        sorted_indices = similarities.argsort()[::-1][:self.TOP_K_SECTIONS]
        
        # è¿”å›æ£€ç´¢ç»“æœ
        results = []
        for idx in sorted_indices:
            doc_id, section_id, anchor, summary = self.section_info[idx]
            results.append({
                'doc_id': doc_id,
                'section_id': section_id,
                'anchor': anchor,
                'summary': summary,
                'score': similarities[idx]
            })
        
        return results
    
    def _search_details(self, query, section_id):
        """åœ¨æŒ‡å®šç« èŠ‚çš„ç»†èŠ‚åº“ä¸­æ£€ç´¢ç›¸å…³æ®µè½"""
        if section_id not in self.detail_corpus:
            return []
        
        # è·å–è¯¥ç« èŠ‚çš„æ‰€æœ‰æ®µè½
        paragraphs = self.detail_corpus[section_id]
        
        # æå–æ®µè½æ–‡æœ¬
        paragraph_texts = [para['content'] for para in paragraphs]
        
        # å¯¹æ®µè½è¿›è¡Œå‘é‡åŒ–
        try:
            # å°è¯•ä½¿ç”¨å·²æœ‰çš„vectorizerè¿›è¡Œè½¬æ¢
            paragraph_matrix = self.vectorizer.transform(paragraph_texts)
        except ValueError:
            # å¦‚æœæœ‰æœªè§è¿‡çš„è¯æ±‡ï¼Œåˆ›å»ºä¸´æ—¶vectorizer
            temp_vectorizer = TfidfVectorizer(
                tokenizer=self._tokenize_chinese,
                analyzer='word',
                lowercase=False,
                vocabulary=self.vectorizer.vocabulary_  # ä½¿ç”¨å·²æœ‰è¯æ±‡è¡¨
            )
            paragraph_matrix = temp_vectorizer.fit_transform(paragraph_texts)
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
        query_vec = self.vectorizer.transform([query])
        
        # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰æ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vec, paragraph_matrix).flatten()
        
        # è·å–æ’åºåçš„ç´¢å¼•
        sorted_indices = similarities.argsort()[::-1][:self.TOP_K_DETAILS]
        
        # è¿”å›æ£€ç´¢ç»“æœ
        results = []
        for idx in sorted_indices:
            para = paragraphs[idx]
            results.append({
                'doc_id': "docA",  # ç¤ºä¾‹ä¸­æ‰€æœ‰æ®µè½éƒ½æ¥è‡ªåŒä¸€æ–‡æ¡£
                'section_id': section_id,
                'paragraph_id': para['id'],
                'anchor': para['anchor'],
                'content': para['content'],
                'score': similarities[idx]
            })
        
        return results
    
    def _compute_consistency(self, summary_text, detail_paragraphs):
        """è®¡ç®—æ‘˜è¦ä¸ç»†èŠ‚æ®µè½ä¹‹é—´çš„ä¸€è‡´æ€§å¾—åˆ†"""
        if not detail_paragraphs:
            return 0.0
        
        # å¯¹æ‘˜è¦è¿›è¡Œå‘é‡åŒ–
        summary_vec = self.vectorizer.transform([summary_text])
        
        # æå–ç»†èŠ‚æ®µè½æ–‡æœ¬å¹¶å‘é‡åŒ–
        detail_texts = [para['content'] for para in detail_paragraphs]
        detail_matrix = self.vectorizer.transform(detail_texts)
        
        # è®¡ç®—æ‘˜è¦ä¸æ¯ä¸ªç»†èŠ‚æ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(summary_vec, detail_matrix).flatten()
        
        # è¿”å›å¹³å‡ä¸€è‡´æ€§å¾—åˆ†
        return np.mean(similarities)
    
    def _fallback_to_full_text(self, query):
        """å›é€€ç­–ç•¥ï¼šå¯¹å…¨å±€æ®µè½åº“è¿›è¡Œé‡æ’"""
        # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
        query_vec = self.vectorizer.transform([query])
        
        # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰å…¨å±€æ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vec, self.global_matrix).flatten()
        
        # è·å–æ’åºåçš„ç´¢å¼•
        sorted_indices = similarities.argsort()[::-1][:self.TOP_K_FINAL]
        
        # è¿”å›é‡æ’ç»“æœ
        results = []
        for idx in sorted_indices:
            para_info = self.global_paragraph_info[idx].copy()
            para_info['score'] = similarities[idx]
            results.append(para_info)
        
        return results
    
    def consistency_gate_retrieval(self, query):
        """
        æ‰§è¡Œå¸¦ä¸€è‡´æ€§é˜ˆå€¼å®ˆé—¨çš„æ£€ç´¢æµç¨‹
        1. åœ¨æ‘˜è¦åº“ä¸­æ£€ç´¢ç›¸å…³ç« èŠ‚
        2. åœ¨æ£€ç´¢åˆ°çš„ç« èŠ‚çš„ç»†èŠ‚åº“ä¸­æ£€ç´¢ç›¸å…³æ®µè½
        3. è®¡ç®—æ‘˜è¦ä¸ç»†èŠ‚çš„ä¸€è‡´æ€§å¾—åˆ†
        4. å¦‚æœä¸€è‡´æ€§å¾—åˆ†ä½äºé˜ˆå€¼ï¼Œè§¦å‘å›é€€ç­–ç•¥
        5. è¿”å›æœ€ç»ˆæ£€ç´¢ç»“æœ
        """
        start_time = time.time()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ‘˜è¦æ£€ç´¢
        section_results = self._search_summary(query)
        
        # åˆå§‹åŒ–ç»“æœ
        results = {
            'query': query,
            'sections': section_results,
            'details': [],
            'consistency_score': 0.0,
            'fallback_triggered': False,
            'final_results': [],
            'execution_time': 0.0
        }
        
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°ç« èŠ‚ï¼Œç›´æ¥å›é€€
        if not section_results:
            results['fallback_triggered'] = True
            results['final_results'] = self._fallback_to_full_text(query)
        else:
            # ç¬¬äºŒé˜¶æ®µï¼šç»†èŠ‚æ£€ç´¢
            section = section_results[0]  # åªå¤„ç†top-1ç« èŠ‚
            section_id = section['section_id']
            detail_results = self._search_details(query, section_id)
            results['details'] = detail_results
            
            # è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†
            consistency_score = self._compute_consistency(section['summary'], detail_results)
            results['consistency_score'] = consistency_score
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å›é€€
            if consistency_score < self.TAU:
                results['fallback_triggered'] = True
                results['final_results'] = self._fallback_to_full_text(query)
            else:
                # ä¸éœ€è¦å›é€€ï¼Œä½¿ç”¨ç»†èŠ‚æ£€ç´¢ç»“æœä½œä¸ºæœ€ç»ˆç»“æœ
                results['final_results'] = detail_results[:self.TOP_K_FINAL]  # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        
        end_time = time.time()
        results['execution_time'] = end_time - start_time
        
        return results
    
    def format_results(self, results):
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœä»¥ä¾¿å±•ç¤º"""
        formatted_output = []
        
        # è¾“å‡ºæŸ¥è¯¢
        formatted_output.append(f"æŸ¥è¯¢: {results['query']}")
        formatted_output.append("=" * 60)
        
        # è¾“å‡ºç« èŠ‚æ£€ç´¢ç»“æœ
        if results['sections']:
            section = results['sections'][0]  # åªå±•ç¤ºtop-1ç« èŠ‚
            consistency_str = f"Consistency={results['consistency_score']:.2f} < {self.TAU:.2f} â†’ å›é€€" if results['fallback_triggered'] else f"Consistency={results['consistency_score']:.2f} â‰¥ {self.TAU:.2f} â†’ ä¿ç•™"
            formatted_output.append(f"Top section: ({section['doc_id']}, {section['section_id']}), {consistency_str}")
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        formatted_output.append("Final:")
        for i, result in enumerate(results['final_results'], 1):
            # æˆªæ–­é•¿æ–‡æœ¬ä»¥ä¾¿å±•ç¤º
            content_preview = result['content'][:50] + ('...' if len(result['content']) > 50 else '')
            formatted_output.append(f"  [{i}] ({result['doc_id']}, {result['section_id']}, {result['anchor']}) {result['score']:.2f} â†’ \"{content_preview}\"")
        
        # è¾“å‡ºæ‰§è¡Œæ—¶é—´
        formatted_output.append(f"æ‰§è¡Œæ—¶é—´: {results['execution_time']:.4f}ç§’")
        
        return '\n'.join(formatted_output)
    
    def save_results(self, results, filename="consistency_gate_results.json"):
        """ä¿å­˜æ£€ç´¢ç»“æœåˆ°æ–‡ä»¶"""
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ æ£€ç´¢ç»“æœå·²ä¿å­˜è‡³: {filename}")


# ç¤ºä¾‹æŸ¥è¯¢ - åŒ…å«å¯èƒ½è§¦å‘å›é€€å’Œä¸è§¦å‘å›é€€çš„æƒ…å†µ
SAMPLE_QUERIES = [
    "æ··åˆæ£€ç´¢ç­–ç•¥çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",  # å¯èƒ½è§¦å‘å›é€€çš„æŸ¥è¯¢
    "æ–‡æœ¬åˆ†å—çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ",       # å¯èƒ½ä¿ç•™çš„æŸ¥è¯¢
    "RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ä»€ä¹ˆï¼Ÿ",    # å¯èƒ½ä¿ç•™çš„æŸ¥è¯¢
    "ä»€ä¹ˆæ˜¯æ··åˆæ£€ç´¢çš„æœ€ä½³å®è·µï¼Ÿ"      # æ–°çš„æŸ¥è¯¢ï¼Œå¯èƒ½è§¦å‘å›é€€
]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ä¸€è‡´æ€§é˜ˆå€¼å®ˆé—¨ä¸å›é€€å·¥å…·")
    print(f"ğŸ”§ é…ç½®å‚æ•°: TAU={ConsistencyGateAndFallback.TAU}, TOP_K_SECTIONS={ConsistencyGateAndFallback.TOP_K_SECTIONS}, ")
    print(f"              TOP_K_DETAILS={ConsistencyGateAndFallback.TOP_K_DETAILS}, TOP_K_FINAL={ConsistencyGateAndFallback.TOP_K_FINAL}")
    
    # åˆ›å»ºæ£€ç´¢å™¨å®ä¾‹
    retriever = ConsistencyGateAndFallback()
    
    # æ‰§è¡Œç¤ºä¾‹æŸ¥è¯¢
    for i, query in enumerate(SAMPLE_QUERIES, 1):
        print(f"\nğŸ” ç¤ºä¾‹æŸ¥è¯¢ {i}/{len(SAMPLE_QUERIES)}: {query}")
        
        # æ‰§è¡Œå¸¦ä¸€è‡´æ€§é˜ˆå€¼å®ˆé—¨çš„æ£€ç´¢
        results = retriever.consistency_gate_retrieval(query)
        
        # æ ¼å¼åŒ–å¹¶æ‰“å°ç»“æœ
        formatted_results = retriever.format_results(results)
        print(formatted_results)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        retriever.save_results(results, f"consistency_gate_results_{i}.json")
    
    # æ£€æŸ¥ä¸­æ–‡è¾“å‡º
    print("\nğŸ” ä¸­æ–‡è¾“å‡ºæµ‹è¯•ï¼šä¸€è‡´æ€§é˜ˆå€¼å®ˆé—¨ä¸å›é€€ç³»ç»ŸæˆåŠŸå®ç°æ‘˜è¦ä¸ç»†èŠ‚ä¸€è‡´æ€§æ ¡éªŒåŠŸèƒ½")
    
    print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")


if __name__ == "__main__":
    main()