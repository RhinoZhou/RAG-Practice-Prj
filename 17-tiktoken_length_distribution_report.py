#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Tiktoken ç²¾å‡†è®¡æ•°ä¸åˆ†å¸ƒæŠ¥å‘Šç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
- é€æ®µç»Ÿè®¡æ–‡æœ¬çš„tokené•¿åº¦åˆ†å¸ƒ
- è¾“å‡ºP50/P90/P99ç­‰ç™¾åˆ†ä½æ•°ç»Ÿè®¡
- è¯†åˆ«å¹¶æŠ¥å‘Šé•¿å°¾æ ·æœ¬ï¼ˆè¶…é•¿æ®µè½ï¼‰
- å¤„ç†å¼‚å¸¸å­—ç¬¦å’Œè¡¨æƒ…ç¬¦å·
- ç”ŸæˆJSONå’ŒCSVæ ¼å¼çš„ç»Ÿè®¡æŠ¥å‘Š

ç”¨æ³•ç¤ºä¾‹ï¼š
```python
from tiktoken_length_distribution_report import TiktokenLengthDistributionReport

# åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨å®ä¾‹
reporter = TiktokenLengthDistributionReport(tokenizer_name="cl100k_base")

# å¤„ç†æ–‡æœ¬åˆ—è¡¨
texts = ["æ®µè½1å†…å®¹...", "æ®µè½2å†…å®¹...", ...]
results = reporter.analyze_texts(texts)

# æˆ–è€…å¤„ç†JSONæ ¼å¼çš„å—åˆ—è¡¨
chunks = [{"text": "å—1å†…å®¹"}, {"text": "å—2å†…å®¹"}, ...]
results = reporter.analyze_chunks(chunks)
```
"""

import json
import csv
import re
import numpy as np
import os
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Any
from pathlib import Path
import time

# å°è¯•å¯¼å…¥tiktokenåº“
HAS_TIKTOKEN = False
try:
    import tiktoken
    HAS_TIKTOKEN = True
except ImportError:
    print("è­¦å‘Š: tiktokenåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„å­—ç¬¦è®¡æ•°ä½œä¸ºæ›¿ä»£ã€‚")
    print("å»ºè®®å®‰è£…tiktokenä»¥è·å¾—æ›´å‡†ç¡®çš„tokenè®¡æ•°: pip install tiktoken")


@dataclass
class TokenStatistics:
    """å­˜å‚¨å•ä¸ªæ–‡æœ¬æ®µçš„tokenç»Ÿè®¡ä¿¡æ¯"""
    text: str  # åŸå§‹æ–‡æœ¬
    token_count: int  # tokenæ•°é‡
    char_count: int  # å­—ç¬¦æ•°é‡
    has_emoji: bool = False  # æ˜¯å¦åŒ…å«è¡¨æƒ…ç¬¦å·
    has_special_chars: bool = False  # æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦
    issues: List[str] = field(default_factory=list)  # ç¼–ç é—®é¢˜åˆ—è¡¨


@dataclass
class DistributionReport:
    """å­˜å‚¨æ•´ä½“tokené•¿åº¦åˆ†å¸ƒæŠ¥å‘Š"""
    total_segments: int  # æ€»æ®µæ•°
    total_tokens: int  # æ€»tokenæ•°
    total_chars: int  # æ€»å­—ç¬¦æ•°
    avg_tokens_per_segment: float  # æ¯æ®µå¹³å‡tokenæ•°
    p50: int  # 50%åˆ†ä½æ•°
    p90: int  # 90%åˆ†ä½æ•°
    p99: int  # 99%åˆ†ä½æ•°
    max_tokens: int  # æœ€å¤§tokenæ•°
    min_tokens: int  # æœ€å°tokenæ•°
    long_tail_segments: List[Dict[str, Any]] = field(default_factory=list)  # é•¿å°¾æ ·æœ¬åˆ—è¡¨
    distribution_data: Dict[str, List[float]] = field(default_factory=dict)  # åˆ†å¸ƒæ•°æ®
    processing_time: float = 0.0  # å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
    tokenizer_used: str = ""  # ä½¿ç”¨çš„tokenizer
    timestamp: str = ""  # å¤„ç†æ—¶é—´æˆ³


class SimpleCharacterTokenizer:
    """ç®€å•çš„å­—ç¬¦tokenizerå®ç°ï¼Œç”¨äºåœ¨tiktokenä¸å¯ç”¨æ—¶ä½œä¸ºæ›¿ä»£"""
    
    def tokenize(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºå­—ç¬¦çº§åˆ«token"""
        return list(text)
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.tokenize(text))


class TiktokenLengthDistributionReport:
    """Tiktokené•¿åº¦åˆ†å¸ƒæŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, tokenizer_name: str = "cl100k_base", use_simple_tokenizer: bool = False, 
                 long_tail_threshold: float = 0.95, max_long_tail_samples: int = 20):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        å‚æ•°:
            tokenizer_name: ä½¿ç”¨çš„tiktokenç¼–ç åç§°ï¼Œé»˜è®¤ä¸ºcl100k_baseï¼ˆGPT-4ä½¿ç”¨çš„ç¼–ç ï¼‰
            use_simple_tokenizer: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨ç®€å•çš„å­—ç¬¦tokenizer
            long_tail_threshold: é•¿å°¾æ ·æœ¬çš„é˜ˆå€¼ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤95%ï¼Œå³åªä¿ç•™é•¿åº¦åœ¨å‰5%çš„æ ·æœ¬ï¼‰
            max_long_tail_samples: æœ€å¤šä¿ç•™çš„é•¿å°¾æ ·æœ¬æ•°é‡
        """
        self.tokenizer_name = tokenizer_name
        self.use_simple_tokenizer = use_simple_tokenizer or not HAS_TIKTOKEN
        self.long_tail_threshold = long_tail_threshold
        self.max_long_tail_samples = max_long_tail_samples
        self.tokenizer = self._load_tokenizer()
        
        # ç”¨äºæ£€æµ‹è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
        self.emoji_pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', 
                                       re.UNICODE)
        self.special_char_pattern = re.compile(r'[^\w\s\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;:"\'\-]')
    
    def _load_tokenizer(self):
        """åŠ è½½æŒ‡å®šçš„tokenizer"""
        if self.use_simple_tokenizer:
            print("ä½¿ç”¨ç®€å•çš„å­—ç¬¦tokenizerè¿›è¡Œè®¡æ•°")
            return SimpleCharacterTokenizer()
        
        if not HAS_TIKTOKEN:
            print("tiktokenåº“ä¸å¯ç”¨ï¼Œè‡ªåŠ¨ä½¿ç”¨ç®€å•çš„å­—ç¬¦tokenizer")
            return SimpleCharacterTokenizer()
        
        try:
            print(f"åŠ è½½tiktokenç¼–ç : {self.tokenizer_name}")
            return tiktoken.get_encoding(self.tokenizer_name)
        except Exception as e:
            print(f"åŠ è½½tiktokenå¤±è´¥: {e}")
            print("åˆ‡æ¢åˆ°ç®€å•çš„å­—ç¬¦tokenizer")
            return SimpleCharacterTokenizer()
    
    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬
            
        è¿”å›:
            tokenæ•°é‡
        """
        if isinstance(self.tokenizer, SimpleCharacterTokenizer):
            return self.tokenizer.count_tokens(text)
        else:
            try:
                # ä½¿ç”¨tiktokençš„encodeæ–¹æ³•è®¡ç®—tokenæ•°é‡ï¼Œå¿½ç•¥ä¸å…è®¸çš„ç‰¹æ®Šå­—ç¬¦
                return len(self.tokenizer.encode(text, disallowed_special=()))
            except Exception as e:
                print(f"ç¼–ç é”™è¯¯: {e}ï¼Œä½¿ç”¨å­—ç¬¦è®¡æ•°ä½œä¸ºæ›¿ä»£")
                return len(text)
    
    def _detect_issues(self, text: str) -> Tuple[bool, bool, List[str]]:
        """æ£€æµ‹æ–‡æœ¬ä¸­çš„è¡¨æƒ…ç¬¦å·ã€ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç é—®é¢˜
        
        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬
            
        è¿”å›:
            (æ˜¯å¦åŒ…å«è¡¨æƒ…ç¬¦å·, æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦, é—®é¢˜åˆ—è¡¨)
        """
        has_emoji = bool(self.emoji_pattern.search(text))
        has_special_chars = bool(self.special_char_pattern.search(text))
        issues = []
        
        if has_emoji:
            issues.append("åŒ…å«è¡¨æƒ…ç¬¦å·")
        
        if has_special_chars:
            issues.append("åŒ…å«ç‰¹æ®Šå­—ç¬¦")
        
        # æ£€æµ‹å¯èƒ½çš„ç¼–ç é—®é¢˜
        try:
            text.encode('utf-8').decode('utf-8')
        except UnicodeDecodeError:
            issues.append("å­˜åœ¨ç¼–ç é—®é¢˜")
        
        return has_emoji, has_special_chars, issues
    
    def _analyze_single_text(self, text: str, segment_id: int = 0) -> TokenStatistics:
        """åˆ†æå•ä¸ªæ–‡æœ¬æ®µçš„tokenç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬æ®µ
            segment_id: æ®µID
            
        è¿”å›:
            TokenStatisticså¯¹è±¡
        """
        # æ¸…ç†æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦ï¼‰
        cleaned_text = re.sub(r'\s+', ' ', text).strip()
        
        # è®¡ç®—tokenæ•°é‡å’Œå­—ç¬¦æ•°é‡
        token_count = self._count_tokens(cleaned_text)
        char_count = len(cleaned_text)
        
        # æ£€æµ‹é—®é¢˜
        has_emoji, has_special_chars, issues = self._detect_issues(cleaned_text)
        
        return TokenStatistics(
            text=cleaned_text,
            token_count=token_count,
            char_count=char_count,
            has_emoji=has_emoji,
            has_special_chars=has_special_chars,
            issues=issues
        )
    
    def analyze_texts(self, texts: List[str]) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬åˆ—è¡¨çš„tokené•¿åº¦åˆ†å¸ƒ
        
        å‚æ•°:
            texts: æ–‡æœ¬æ®µåˆ—è¡¨
            
        è¿”å›:
            åŒ…å«ç»Ÿè®¡æŠ¥å‘Šçš„å­—å…¸
        """
        start_time = time.time()
        
        # åˆ†ææ¯ä¸ªæ–‡æœ¬æ®µ
        all_statistics = []
        token_counts = []
        
        for i, text in enumerate(texts):
            if not text or not text.strip():
                continue  # è·³è¿‡ç©ºæ–‡æœ¬
            
            stats = self._analyze_single_text(text, i)
            all_statistics.append(stats)
            token_counts.append(stats.token_count)
        
        # ç”Ÿæˆåˆ†å¸ƒæŠ¥å‘Š
        report = self._generate_report(all_statistics, token_counts)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        report.processing_time = time.time() - start_time
        report.tokenizer_used = "tiktoken (" + self.tokenizer_name + ")" if HAS_TIKTOKEN and not self.use_simple_tokenizer else "ç®€å•å­—ç¬¦è®¡æ•°"
        report.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        
        return {
            "metadata": {
                "total_segments": report.total_segments,
                "total_tokens": report.total_tokens,
                "total_chars": report.total_chars,
                "avg_tokens_per_segment": report.avg_tokens_per_segment,
                "p50": report.p50,
                "p90": report.p90,
                "p99": report.p99,
                "max_tokens": report.max_tokens,
                "min_tokens": report.min_tokens,
                "tokenizer_used": report.tokenizer_used,
                "processing_time": report.processing_time,
                "timestamp": report.timestamp
            },
            "distribution_data": report.distribution_data,
            "long_tail_segments": report.long_tail_segments,
            "detailed_statistics": [{
                "text": stat.text,
                "token_count": stat.token_count,
                "char_count": stat.char_count,
                "has_emoji": stat.has_emoji,
                "has_special_chars": stat.has_special_chars,
                "issues": stat.issues
            } for stat in all_statistics]
        }
    
    def analyze_chunks(self, chunks: List[Dict[str, Any]], text_key: str = "text") -> Dict[str, Any]:
        """åˆ†æå—åˆ—è¡¨çš„tokené•¿åº¦åˆ†å¸ƒ
        
        å‚æ•°:
            chunks: å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—æ˜¯åŒ…å«textå­—æ®µçš„å­—å…¸
            text_key: å—ä¸­åŒ…å«æ–‡æœ¬çš„é”®å
            
        è¿”å›:
            åŒ…å«ç»Ÿè®¡æŠ¥å‘Šçš„å­—å…¸
        """
        # æå–æ–‡æœ¬åˆ—è¡¨
        texts = []
        for chunk in chunks:
            if isinstance(chunk, dict) and text_key in chunk and chunk[text_key]:
                texts.append(chunk[text_key])
        
        # è°ƒç”¨analyze_textsè¿›è¡Œåˆ†æ
        return self.analyze_texts(texts)
    
    def _generate_report(self, all_statistics: List[TokenStatistics], token_counts: List[int]) -> DistributionReport:
        """ç”Ÿæˆtokené•¿åº¦åˆ†å¸ƒæŠ¥å‘Š
        
        å‚æ•°:
            all_statistics: æ‰€æœ‰æ–‡æœ¬æ®µçš„ç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨
            token_counts: æ‰€æœ‰æ–‡æœ¬æ®µçš„tokenæ•°é‡åˆ—è¡¨
            
        è¿”å›:
            DistributionReportå¯¹è±¡
        """
        if not token_counts:
            return DistributionReport(
                total_segments=0,
                total_tokens=0,
                total_chars=0,
                avg_tokens_per_segment=0,
                p50=0,
                p90=0,
                p99=0,
                max_tokens=0,
                min_tokens=0
            )
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        total_segments = len(all_statistics)
        total_tokens = sum(token_counts)
        total_chars = sum(stat.char_count for stat in all_statistics)
        avg_tokens_per_segment = total_tokens / total_segments if total_segments > 0 else 0
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        p50 = int(np.percentile(token_counts, 50))
        p90 = int(np.percentile(token_counts, 90))
        p99 = int(np.percentile(token_counts, 99))
        max_tokens = max(token_counts)
        min_tokens = min(token_counts)
        
        # ç”Ÿæˆåˆ†å¸ƒæ•°æ®ï¼ˆæŒ‰tokenæ•°é‡åˆ†ç»„ï¼‰
        distribution_data = self._generate_distribution_data(token_counts)
        
        # è¯†åˆ«é•¿å°¾æ ·æœ¬
        long_tail_segments = self._identify_long_tail_segments(all_statistics, token_counts)
        
        return DistributionReport(
            total_segments=total_segments,
            total_tokens=total_tokens,
            total_chars=total_chars,
            avg_tokens_per_segment=avg_tokens_per_segment,
            p50=p50,
            p90=p90,
            p99=p99,
            max_tokens=max_tokens,
            min_tokens=min_tokens,
            distribution_data=distribution_data,
            long_tail_segments=long_tail_segments
        )
    
    def _generate_distribution_data(self, token_counts: List[int]) -> Dict[str, List[float]]:
        """ç”Ÿæˆtokenæ•°é‡åˆ†å¸ƒæ•°æ®
        
        å‚æ•°:
            token_counts: tokenæ•°é‡åˆ—è¡¨
            
        è¿”å›:
            åŒ…å«åˆ†å¸ƒæ•°æ®çš„å­—å…¸ï¼ŒåŒ…æ‹¬åŒºé—´ã€è®¡æ•°ã€ç™¾åˆ†æ¯”å’Œç´¯ç§¯ç™¾åˆ†æ¯”
        """
        # åˆ›å»ºç­‰å®½çš„binï¼šæ ¹æ®æœ€å¤§tokenæ•°ï¼Œæœ€å¤šåˆ›å»º20ä¸ªbinï¼Œç¡®ä¿binå®½åº¦è‡³å°‘ä¸º1
        max_count = max(token_counts)
        bin_width = max(1, int(max_count / 20))  # æœ€å¤š20ä¸ªbin
        bins = list(range(0, max_count + bin_width, bin_width))
        
        # ä½¿ç”¨numpyçš„histogramå‡½æ•°è®¡ç®—ç›´æ–¹å›¾
        # hist: æ¯ä¸ªbinä¸­çš„æ ·æœ¬æ•°é‡
        # bin_edges: å„ä¸ªbinçš„è¾¹ç•Œå€¼
        hist, bin_edges = np.histogram(token_counts, bins=bins)
        
        # å°†è®¡æ•°è½¬æ¢ä¸ºç™¾åˆ†æ¯”ï¼Œä¾¿äºç›´è§‚ç†è§£åˆ†å¸ƒæƒ…å†µ
        total = len(token_counts)
        hist_percent = [count / total * 100 for count in hist]
        
        # è®¡ç®—ç´¯ç§¯åˆ†å¸ƒï¼Œç”¨äºç¡®å®šé•¿å°¾æ ·æœ¬çš„ä½ç½®
        cum_hist = np.cumsum(hist_percent)
        
        # è¿”å›æ ¼å¼åŒ–çš„åˆ†å¸ƒæ•°æ®ï¼Œä¾¿äºåç»­å±•ç¤ºå’Œä¿å­˜
        return {
            "bins": [f"{int(bin_edges[i])}-{int(bin_edges[i+1])}" for i in range(len(bin_edges)-1)],  # tokenåŒºé—´
            "counts": hist.tolist(),  # æ¯ä¸ªåŒºé—´çš„æ ·æœ¬æ•°é‡
            "percentages": hist_percent,  # æ¯ä¸ªåŒºé—´çš„æ ·æœ¬ç™¾åˆ†æ¯”
            "cumulative_percentages": cum_hist.tolist()  # ç´¯ç§¯ç™¾åˆ†æ¯”
        }
    
    def _identify_long_tail_segments(self, all_statistics: List[TokenStatistics], 
                                   token_counts: List[int]) -> List[Dict[str, Any]]:
        """è¯†åˆ«é•¿å°¾æ ·æœ¬ï¼ˆè¶…é•¿æ®µè½ï¼‰
        
        å‚æ•°:
            all_statistics: æ‰€æœ‰æ–‡æœ¬æ®µçš„ç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨
            token_counts: æ‰€æœ‰æ–‡æœ¬æ®µçš„tokenæ•°é‡åˆ—è¡¨
            
        è¿”å›:
            é•¿å°¾æ ·æœ¬åˆ—è¡¨ï¼ŒåŒ…å«ç´¢å¼•ã€tokenæ•°ã€ç‰¹å¾å’Œæ–‡æœ¬é¢„è§ˆ
        """
        if not token_counts:
            return []
        
        # è®¡ç®—é•¿å°¾é˜ˆå€¼ï¼šæ ¹æ®é…ç½®çš„é˜ˆå€¼ç™¾åˆ†æ¯”ç¡®å®š
        threshold = np.percentile(token_counts, self.long_tail_threshold * 100)
        
        # ç­›é€‰å‡ºè¶…è¿‡é˜ˆå€¼çš„æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å±äºåˆ†å¸ƒçš„é•¿å°¾éƒ¨åˆ†
        long_tail_indices = [i for i, count in enumerate(token_counts) if count >= threshold]
        
        # æŒ‰tokenæ•°é‡é™åºæ’åºå¹¶å–å‰Nä¸ªï¼Œä¾¿äºåç»­åˆ†æå’Œå±•ç¤º
        
        # æ„å»ºç»“æœåˆ—è¡¨ï¼ŒåŒ…å«æ¯ä¸ªé•¿å°¾æ ·æœ¬çš„å…³é”®ä¿¡æ¯
        long_tail_segments = []
        for i in long_tail_indices:
            stat = all_statistics[i]
            # ä¸ºæ¯ä¸ªé•¿å°¾æ ·æœ¬åˆ›å»ºè¯¦ç»†ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ç´¢å¼•ã€é•¿åº¦ã€ç‰¹å¾å’Œæ–‡æœ¬é¢„è§ˆ
            long_tail_segments.append({
                "segment_index": i,  # æ ·æœ¬åœ¨åŸåˆ—è¡¨ä¸­çš„ç´¢å¼•
                "token_count": stat.token_count,  # tokenæ•°é‡
                "char_count": stat.char_count,  # å­—ç¬¦æ•°é‡
                "has_emoji": stat.has_emoji,  # æ˜¯å¦åŒ…å«è¡¨æƒ…ç¬¦å·
                "has_special_chars": stat.has_special_chars,  # æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦
                "issues": stat.issues,  # æ£€æµ‹åˆ°çš„é—®é¢˜
                "text_preview": stat.text[:200] + ("..." if len(stat.text) > 200 else "")  # æ–‡æœ¬é¢„è§ˆï¼ˆæœ€å¤š200å­—ç¬¦ï¼‰
            })
        
        return long_tail_segments
    
    def save_report_to_json(self, results: Dict[str, Any], output_path: str) -> None:
        """å°†ç»Ÿè®¡æŠ¥å‘Šä¿å­˜ä¸ºJSONæ–‡ä»¶
        
        å‚æ•°:
            results: åˆ†æç»“æœå­—å…¸
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # ä¿å­˜JSONæ–‡ä»¶ï¼Œä½¿ç”¨ensure_ascii=Falseä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œindent=2æ ¼å¼åŒ–è¾“å‡º
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")
    
    def save_report_to_csv(self, results: Dict[str, Any], output_path: str) -> None:
        """å°†ç»Ÿè®¡æŠ¥å‘Šä¿å­˜ä¸ºCSVæ–‡ä»¶
        
        å‚æ•°:
            results: åˆ†æç»“æœå­—å…¸
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯¦ç»†ç»Ÿè®¡æ•°æ®ï¼Œè¿™æ˜¯ç”ŸæˆCSVæŠ¥å‘Šçš„å¿…è¦æ¡ä»¶
        if "detailed_statistics" not in results:
            print("é”™è¯¯: ç»“æœä¸­æ²¡æœ‰è¯¦ç»†ç»Ÿè®¡æ•°æ®")
            return
        
        # ä¿å­˜CSVæ–‡ä»¶ï¼Œä½¿ç”¨å¸¦æœ‰BOMçš„UTF-8ç¼–ç ä»¥è§£å†³Excelä¸­æ–‡ä¹±ç é—®é¢˜
        with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´ï¼Œå®šä¹‰æ¯ä¸€åˆ—çš„åç§°
            writer.writerow(["segment_index", "token_count", "char_count", "has_emoji", 
                            "has_special_chars", "issues", "text_preview"])
            
            # å†™å…¥æ•°æ®ï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬æ®µåˆ›å»ºä¸€è¡Œæ•°æ®
            for i, stat in enumerate(results["detailed_statistics"]):
                writer.writerow([
                    i,  # æ®µç´¢å¼•
                    stat["token_count"],  # tokenæ•°é‡
                    stat["char_count"],  # å­—ç¬¦æ•°é‡
                    "æ˜¯" if stat["has_emoji"] else "å¦",  # è½¬æ¢ä¸ºä¸­æ–‡è¾“å‡º
                    "æ˜¯" if stat["has_special_chars"] else "å¦",  # è½¬æ¢ä¸ºä¸­æ–‡è¾“å‡º
                    ",".join(stat["issues"]) if stat["issues"] else "æ— ",  # é—®é¢˜åˆ—è¡¨
                    stat["text"][:200] + ("..." if len(stat["text"]) > 200 else "")  # æ–‡æœ¬é¢„è§ˆ
                ])
        
        print(f"CSVæŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")
    
    def print_summary_report(self, results: Dict[str, Any]) -> None:
        """æ‰“å°ç»Ÿè®¡æŠ¥å‘Šæ‘˜è¦ï¼Œåœ¨æ§åˆ¶å°è¾“å‡ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            results: åˆ†æç»“æœå­—å…¸
        """
        metadata = results.get("metadata", {})
        
        print("=" * 80)
        print("Tiktoken é•¿åº¦åˆ†å¸ƒç»Ÿè®¡æŠ¥å‘Šæ‘˜è¦")
        print("=" * 80)
        print(f"æ€»æ®µæ•°: {metadata.get('total_segments', 0)}")
        print(f"æ€»tokenæ•°: {metadata.get('total_tokens', 0)}")
        print(f"æ€»å­—ç¬¦æ•°: {metadata.get('total_chars', 0)}")
        print(f"æ¯æ®µå¹³å‡tokenæ•°: {metadata.get('avg_tokens_per_segment', 0):.2f}")
        print(f"\nç™¾åˆ†ä½æ•°ç»Ÿè®¡:")
        print(f"  P50 (ä¸­ä½æ•°): {metadata.get('p50', 0)} tokens")
        print(f"  P90: {metadata.get('p90', 0)} tokens")
        print(f"  P99: {metadata.get('p99', 0)} tokens")
        print(f"\næå€¼ç»Ÿè®¡:")
        print(f"  æœ€å¤§tokenæ•°: {metadata.get('max_tokens', 0)} tokens")
        print(f"  æœ€å°tokenæ•°: {metadata.get('min_tokens', 0)} tokens")
        print(f"\nå¤„ç†ä¿¡æ¯:")
        print(f"  Tokenizer: {metadata.get('tokenizer_used', '')}")
        print(f"  å¤„ç†æ—¶é—´: {metadata.get('processing_time', 0):.3f} ç§’")
        print(f"  å¤„ç†æ—¶é—´æˆ³: {metadata.get('timestamp', '')}")
        
        # æ‰“å°é•¿å°¾æ ·æœ¬ä¿¡æ¯ï¼šåªæ˜¾ç¤ºå‰5ä¸ªï¼Œå…¶ä½™ç”¨çœç•¥è¡¨ç¤º
        long_tail_segments = results.get("long_tail_segments", [])
        if long_tail_segments:
            print(f"\né•¿å°¾æ ·æœ¬ ({len(long_tail_segments)} ä¸ª):")
            print("=" * 80)
            # åªæ‰“å°å‰5ä¸ªé•¿å°¾æ ·æœ¬ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
            for i, segment in enumerate(long_tail_segments[:5]):
                print(f"[{i+1}] Tokenæ•°: {segment['token_count']}, å­—ç¬¦æ•°: {segment['char_count']}")
                print(f"  é—®é¢˜: {', '.join(segment.get('issues', [])) or 'æ— '}")
                print(f"  æ–‡æœ¬é¢„è§ˆ: {segment['text_preview']}")
                print("-" * 80)
            
            # å¦‚æœæœ‰æ›´å¤šé•¿å°¾æ ·æœ¬ï¼Œæç¤ºç”¨æˆ·
            if len(long_tail_segments) > 5:
                print(f"... è¿˜æœ‰ {len(long_tail_segments) - 5} ä¸ªé•¿å°¾æ ·æœ¬æœªæ˜¾ç¤º")
        
        # æ‰“å°åˆ†å¸ƒæ•°æ®ï¼šæ˜¾ç¤ºtokené•¿åº¦åœ¨ä¸åŒåŒºé—´çš„åˆ†å¸ƒæƒ…å†µ
        distribution_data = results.get("distribution_data", {})
        if distribution_data:
            print(f"\nTokené•¿åº¦åˆ†å¸ƒ:")
            print("åŒºé—´        æ•°é‡    ç™¾åˆ†æ¯”    ç´¯ç§¯ç™¾åˆ†æ¯”")
            print("-" * 80)
            bins = distribution_data.get("bins", [])
            counts = distribution_data.get("counts", [])
            percentages = distribution_data.get("percentages", [])
            cum_percentages = distribution_data.get("cumulative_percentages", [])
            
            # åªæ‰“å°å‰10ä¸ªåŒºé—´ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
            for i in range(min(10, len(bins))):
                print(f"{bins[i]:<10} {counts[i]:<6} {percentages[i]:<8.2f}% {cum_percentages[i]:<8.2f}%")
            
            # å¦‚æœæœ‰æ›´å¤šåŒºé—´ï¼Œæç¤ºç”¨æˆ·
            if len(bins) > 10:
                print(f"... è¿˜æœ‰ {len(bins) - 10} ä¸ªåŒºé—´æœªæ˜¾ç¤º")
        
        print("=" * 80)


# ä¸»å‡½æ•°ç¤ºä¾‹
if __name__ == "__main__":
    """ä¸»å‡½æ•°ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨TiktokenLengthDistributionReportç±»"""
    
    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨å®ä¾‹
    reporter = TiktokenLengthDistributionReport(
        tokenizer_name="cl100k_base",  # GPT-4ä½¿ç”¨çš„tokenizer
        long_tail_threshold=0.9,  # åªä¿ç•™é•¿åº¦åœ¨å‰10%çš„æ ·æœ¬
        max_long_tail_samples=10  # æœ€å¤šä¿ç•™10ä¸ªé•¿å°¾æ ·æœ¬
    )
    
    # ç¤ºä¾‹1: ä»corpus.txtæ–‡ä»¶åŠ è½½æ–‡æœ¬
    corpus_path = "corpus.txt"
    if os.path.exists(corpus_path):
        print(f"ä»æ–‡ä»¶åŠ è½½æ–‡æœ¬: {corpus_path}")
        with open(corpus_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬ï¼ˆå‡è®¾æ®µè½ç”±ä¸¤ä¸ªæˆ–å¤šä¸ªæ¢è¡Œç¬¦åˆ†éš”ï¼‰
        paragraphs = [p.strip() for p in re.split(r'\n\s*\n', text) if p.strip()]
        print(f"åˆ†å‰²å¾—åˆ° {len(paragraphs)} ä¸ªæ®µè½")
        
        # åˆ†ææ®µè½
        results = reporter.analyze_texts(paragraphs)
        
        # æ‰“å°æ‘˜è¦æŠ¥å‘Š
        reporter.print_summary_report(results)
        
        # ä¿å­˜æŠ¥å‘Š
        output_dir = "results"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        reporter.save_report_to_json(results, os.path.join(output_dir, "tiktoken_length_distribution_report.json"))
        reporter.save_report_to_csv(results, os.path.join(output_dir, "tiktoken_length_distribution_report.csv"))
        
    else:
        print(f"æœªæ‰¾åˆ°æ–‡ä»¶: {corpus_path}")
        
        # ç¤ºä¾‹2: ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬
        sample_texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›é€ èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æ™ºèƒ½æœºå™¨ã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œä¸éœ€è¦æ˜ç¡®ç¼–ç¨‹ã€‚",
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººç±»å¤§è„‘çš„æŸäº›åŠŸèƒ½ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦çš„äº¤å‰é¢†åŸŸï¼Œæ—¨åœ¨è®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚ğŸ˜",
            "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»å›¾åƒå’Œè§†é¢‘ä¸­è·å–ä¿¡æ¯ã€‚#è®¡ç®—æœºè§†è§‰#",
        ]
        
        print("ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬è¿›è¡Œåˆ†æ")
        results = reporter.analyze_texts(sample_texts)
        reporter.print_summary_report(results)
        
        # ä¿å­˜ç¤ºä¾‹æŠ¥å‘Š
        output_dir = "results"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        reporter.save_report_to_json(results, os.path.join(output_dir, "tiktoken_length_distribution_report_example.json"))
        reporter.save_report_to_csv(results, os.path.join(output_dir, "tiktoken_length_distribution_report_example.csv"))
        
    print("åˆ†æå®Œæˆï¼")