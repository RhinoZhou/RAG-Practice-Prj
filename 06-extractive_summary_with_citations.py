#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŠ½å–å¼æ‘˜è¦ï¼ˆå¥çº§å¯¹é½ï¼‰

ä½œè€…: Ph.D. Rhino
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¥æœŸ: 2024-01-20

åŠŸèƒ½è¯´æ˜:
åŸºäº TFâ€‘IDF æ‰“åˆ†é€‰å–å…³é”®å¥ï¼Œè¾“å‡ºå¸¦é”šç‚¹çš„æŠ½å–å¼æ‘˜è¦ã€‚

å†…å®¹æ¦‚è¿°:
å°†æ®µè½åˆ‡å¥ï¼ŒæŒ‰ä¸æŸ¥è¯¢çš„è¯å‘é‡é‡å æ‰“åˆ†ï¼Œé€‰å– topâ€‘2 å¥æ‹¼æ¥ä¸ºæ‘˜è¦ï¼Œå¹¶é™„ [doc:sec:sent] å¼•ç”¨ä»¥æ”¯æŒå¯è¿½æº¯å®¡é˜…ï¼Œé™ä½ç”Ÿæˆå¹»è§‰é£é™©ã€‚

ä½¿ç”¨åœºæ™¯:
- RAGç³»ç»Ÿä¸­çš„æ‘˜è¦ç”Ÿæˆæ¨¡å—
- éœ€è¦å¯è¿½æº¯æ€§çš„å­¦æœ¯æˆ–æŠ€æœ¯æ–‡æ¡£æ‘˜è¦
- é™ä½å¤§æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¹»è§‰é£é™©

ä¾èµ–åº“:
- sklearn: ç”¨äºTF-IDFå‘é‡åŒ–å’Œç›¸ä¼¼åº¦è®¡ç®—
- jieba: ç”¨äºä¸­æ–‡åˆ†è¯
- numpy: ç”¨äºæ•°å€¼è®¡ç®—
- re: ç”¨äºæ­£åˆ™è¡¨è¾¾å¼å¤„ç†
"""

# è‡ªåŠ¨å®‰è£…ä¾èµ–åº“
import subprocess
import sys
import time
import re

# å®šä¹‰æ‰€éœ€ä¾èµ–åº“
required_dependencies = [
    'scikit-learn',
    'jieba',
    'numpy'
]


def install_dependencies():
    """æ£€æŸ¥å¹¶è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–åº“"""
    for dependency in required_dependencies:
        try:
            # å°è¯•å¯¼å…¥åº“ä»¥æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
            __import__(dependency.replace('-', '_'))  # å¤„ç†scikit-learnçš„å‘½åå·®å¼‚
            print(f"âœ… ä¾èµ–åº“ '{dependency}' å·²å®‰è£…")
        except ImportError:
            print(f"âš ï¸ ä¾èµ–åº“ '{dependency}' æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            # ä½¿ç”¨pipå®‰è£…ç¼ºå¤±çš„ä¾èµ–
            subprocess.check_call([sys.executable, "-m", "pip", "install", dependency])
            print(f"âœ… ä¾èµ–åº“ '{dependency}' å®‰è£…æˆåŠŸ")


# æ‰§è¡Œä¾èµ–å®‰è£…
if __name__ == "__main__":
    install_dependencies()


# å¯¼å…¥æ‰€éœ€åº“
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import numpy as np


class ExtractiveSummaryWithCitations:
    """æŠ½å–å¼æ‘˜è¦ç”Ÿæˆå™¨ï¼Œæ”¯æŒå¸¦å¼•ç”¨é”šç‚¹çš„å…³é”®å¥é€‰æ‹©"""
    
    # é…ç½®å‚æ•° - å¯åœ¨é¡¶éƒ¨è°ƒæ•´
    TOP_N_SENTENCES = 2  # é€‰å–çš„å…³é”®å¥æ•°é‡
    
    def __init__(self):
        """åˆå§‹åŒ–æŠ½å–å¼æ‘˜è¦ç”Ÿæˆå™¨"""
        # åˆå§‹åŒ–æ•°æ®ç»“æ„
        self.documents = {}
        self.vectorizer = None
        
        # åŠ è½½ç¤ºä¾‹æ•°æ®
        self._load_sample_data()
        
        # åˆå§‹åŒ–TF-IDFæ¨¡å‹
        self._initialize_tfidf()
    
    def _load_sample_data(self):
        """åŠ è½½ç¤ºä¾‹æ–‡æ¡£æ•°æ®"""
        # ç¤ºä¾‹æ–‡æ¡£æ•°æ® - æŠ€æœ¯ç™½çš®ä¹¦çš„éƒ¨åˆ†ç« èŠ‚
        self.documents = {
            "docA": {
                "2.2": [  # ç« èŠ‚2.2çš„å¥å­
                    "æ··åˆæ£€ç´¢ç»“åˆå…³é”®è¯åŒ¹é…ä¸è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚",
                    "å…³é”®è¯åŒ¹é…é€šè¿‡ç²¾ç¡®çš„æ–‡æœ¬åŒ¹é…å¿«é€Ÿå®šä½ç›¸å…³æ–‡æ¡£ï¼Œè€Œè¯­ä¹‰ç›¸ä¼¼æ€§è®¡ç®—åˆ™æ•è·æ·±å±‚å«ä¹‰çš„å…³è”ã€‚",
                    "TF-IDFæ˜¯ä¸€ç§å¸¸ç”¨çš„å…³é”®è¯æ£€ç´¢æ–¹æ³•ï¼Œå®ƒæ ¹æ®è¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡è®¡ç®—è¯çš„é‡è¦æ€§ã€‚",
                    "RRFèåˆå‰åˆ—ç»“æœï¼Œæå‡ç¨³å¥æ€§ã€‚"
                ],
                "3.1": [  # ç« èŠ‚3.1çš„å¥å­
                    "æ–‡æœ¬åˆ†å—æ˜¯å°†é•¿æ–‡æ¡£åˆ†å‰²æˆæ›´å°ã€æ›´æ˜“äºå¤„ç†çš„ç‰‡æ®µçš„è¿‡ç¨‹ã€‚",
                    "è¿™æœ‰åŠ©äºæé«˜æ£€ç´¢çš„å‡†ç¡®æ€§å’Œç”Ÿæˆçš„è´¨é‡ã€‚",
                    "åˆé€‚çš„åˆ†å—å¤§å°éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ–‡æ¡£ç‰¹ç‚¹è¿›è¡Œè°ƒæ•´ã€‚"
                ],
                "3.2": [  # ç« èŠ‚3.2çš„å¥å­
                    "çª—å£æ»‘åŠ¨ç­–ç•¥é€šè¿‡å›ºå®šå¤§å°çš„çª—å£å’Œé‡å éƒ¨åˆ†ï¼Œå°†æ–‡æœ¬åˆ’åˆ†ä¸ºè¿ç»­çš„å—ã€‚",
                    "è¿™ä¿æŒäº†æ–‡æœ¬çš„å±€éƒ¨è¿è´¯æ€§ã€‚",
                    "çª—å£å¤§å°å’Œé‡å æ¯”ä¾‹æ˜¯å½±å“åˆ†å—æ•ˆæœçš„ä¸¤ä¸ªå…³é”®å‚æ•°ã€‚"
                ]
            }
        }
    
    def _tokenize_chinese(self, text):
        """ä¸­æ–‡åˆ†è¯å¤„ç†"""
        # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        return ' '.join(jieba.cut(text))
    
    def _initialize_tfidf(self):
        """åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨"""
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£çš„æ‰€æœ‰å¥å­ç”¨äºè®­ç»ƒTF-IDFæ¨¡å‹
        all_sentences = []
        for doc_id, sections in self.documents.items():
            for section_id, sentences in sections.items():
                all_sentences.extend(sentences)
        
        # åˆ›å»ºä¸­æ–‡TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            tokenizer=self._tokenize_chinese,
            analyzer='word',
            lowercase=False,  # ä¸­æ–‡ä¸åŒºåˆ†å¤§å°å†™
            max_features=5000
        )
        
        # è®­ç»ƒTF-IDFæ¨¡å‹
        self.vectorizer.fit(all_sentences)
    
    def _split_into_sentences(self, text):
        """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­ï¼ˆä¸­æ–‡å¥å­é€šå¸¸ä»¥å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç»“å°¾ï¼‰
        sentences = re.split(r'[ã€‚ï¼Ÿï¼]', text)
        # å»é™¤ç©ºå¥å­
        return [sentence.strip() for sentence in sentences if sentence.strip()]
    
    def generate_summary(self, query):
        """
        ç”Ÿæˆå¸¦å¼•ç”¨çš„æŠ½å–å¼æ‘˜è¦
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        
        è¿”å›:
            dict: åŒ…å«æ‘˜è¦æ–‡æœ¬å’Œå¼•ç”¨åˆ—è¡¨çš„å­—å…¸
        """
        start_time = time.time()
        
        # åˆå§‹åŒ–ç»“æœ
        results = {
            'query': query,
            'summary': '',
            'citations': [],
            'sentence_scores': [],
            'execution_time': 0.0
        }
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
        query_vector = self.vectorizer.transform([self._tokenize_chinese(query)])
        
        # å­˜å‚¨æ‰€æœ‰å¥å­åŠå…¶ä¿¡æ¯
        all_sentences_info = []
        
        # éå†æ‰€æœ‰æ–‡æ¡£å’Œç« èŠ‚
        for doc_id, sections in self.documents.items():
            for section_id, sentences in sections.items():
                for sent_idx, sentence in enumerate(sentences, 1):
                    # å¯¹å¥å­è¿›è¡Œå‘é‡åŒ–
                    sentence_vector = self.vectorizer.transform([self._tokenize_chinese(sentence)])
                    
                    # è®¡ç®—å¥å­ä¸æŸ¥è¯¢çš„ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = cosine_similarity(query_vector, sentence_vector)[0][0]
                    
                    # å­˜å‚¨å¥å­ä¿¡æ¯å’Œç›¸ä¼¼åº¦å¾—åˆ†
                    all_sentences_info.append({
                        'doc_id': doc_id,
                        'section_id': section_id,
                        'sent_idx': sent_idx,
                        'sentence': sentence,
                        'score': similarity
                    })
        
        # æŒ‰ç…§ç›¸ä¼¼åº¦å¾—åˆ†æ’åº
        all_sentences_info.sort(key=lambda x: x['score'], reverse=True)
        
        # è®°å½•å¥å­å¾—åˆ†ä¿¡æ¯
        results['sentence_scores'] = [
            {
                'doc_id': info['doc_id'],
                'section_id': info['section_id'],
                'sentence': info['sentence'][:30] + ('...' if len(info['sentence']) > 30 else ''),
                'score': info['score']
            } for info in all_sentences_info[:5]  # åªè®°å½•å‰5ä¸ªå¥å­
        ]
        
        # é€‰å–top-Nä¸ªå…³é”®å¥
        top_sentences = all_sentences_info[:self.TOP_N_SENTENCES]
        
        # æŒ‰ç…§åœ¨åŸæ–‡ä¸­çš„ä½ç½®é‡æ–°æ’åºï¼ˆä¿æŒåŸæ–‡é€»è¾‘ï¼‰
        top_sentences.sort(key=lambda x: (x['doc_id'], x['section_id'], x['sent_idx']))
        
        # æ„å»ºæ‘˜è¦æ–‡æœ¬å’Œå¼•ç”¨åˆ—è¡¨
        summary_parts = []
        citations = []
        
        for info in top_sentences:
            summary_parts.append(info['sentence'])
            # æ„å»ºå¼•ç”¨æ ¼å¼: [doc:sec:sent]
            citations.append(f"[{info['doc_id']}:{info['section_id']}:s{info['sent_idx']}]")
        
        # æ‹¼æ¥æ‘˜è¦æ–‡æœ¬
        results['summary'] = ' '.join(summary_parts)
        results['citations'] = citations
        
        # è®°å½•æ‰§è¡Œæ—¶é—´
        end_time = time.time()
        results['execution_time'] = end_time - start_time
        
        return results
    
    def save_results(self, results, filename="extractive_summary_results.json"):
        """ä¿å­˜æ£€ç´¢ç»“æœåˆ°æ–‡ä»¶"""
        import json
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ æ£€ç´¢ç»“æœå·²ä¿å­˜è‡³: {filename}")

    def format_results(self, results):
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœä»¥ä¾¿å±•ç¤º"""
        formatted_output = []
        
        # è¾“å‡ºæŸ¥è¯¢
        formatted_output.append(f"æŸ¥è¯¢: {results['query']}")
        formatted_output.append("=" * 60)
        
        # è¾“å‡ºæ‘˜è¦å’Œå¼•ç”¨
        formatted_output.append(f"æ‘˜è¦: {results['summary']}")
        formatted_output.append(f"å¼•ç”¨: {' '.join(results['citations'])}")
        formatted_output.append("=" * 60)
        
        # è¾“å‡ºå¥å­å¾—åˆ†ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        formatted_output.append("Top 5å¥å­å¾—åˆ†:")
        for i, info in enumerate(results['sentence_scores'], 1):
            formatted_output.append(f"  [{i}] ({info['doc_id']}, {info['section_id']}) {info['score']:.3f} â†’ {info['sentence']}")
        
        # è¾“å‡ºæ‰§è¡Œæ—¶é—´
        formatted_output.append(f"æ‰§è¡Œæ—¶é—´: {results['execution_time']:.4f}ç§’")
        
        return '\n'.join(formatted_output)


# ç¤ºä¾‹æŸ¥è¯¢
SAMPLE_QUERIES = [
    "æ··åˆæ£€ç´¢çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æ–‡æœ¬åˆ†å—çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    "çª—å£æ»‘åŠ¨ç­–ç•¥çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æŠ½å–å¼æ‘˜è¦ç”Ÿæˆå·¥å…·")
    print(f"ğŸ”§ é…ç½®å‚æ•°: TOP_N_SENTENCES={ExtractiveSummaryWithCitations.TOP_N_SENTENCES}")
    
    # åˆ›å»ºæ‘˜è¦ç”Ÿæˆå™¨å®ä¾‹
    summarizer = ExtractiveSummaryWithCitations()
    
    # æ‰§è¡Œç¤ºä¾‹æŸ¥è¯¢
    for i, query in enumerate(SAMPLE_QUERIES, 1):
        print(f"\nğŸ” ç¤ºä¾‹æŸ¥è¯¢ {i}/{len(SAMPLE_QUERIES)}: {query}")
        
        # ç”Ÿæˆæ‘˜è¦
        results = summarizer.generate_summary(query)
        
        # æ ¼å¼åŒ–å¹¶æ‰“å°ç»“æœ
        formatted_results = summarizer.format_results(results)
        print(formatted_results)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        result_filename = f"extractive_summary_results_{i}.json"
        summarizer.save_results(results, result_filename)
    
    # æ£€æŸ¥ä¸­æ–‡è¾“å‡º
    print("\nğŸ” ä¸­æ–‡è¾“å‡ºæµ‹è¯•ï¼šæŠ½å–å¼æ‘˜è¦ç”Ÿæˆå·¥å…·æˆåŠŸå®ç°å¸¦å¼•ç”¨é”šç‚¹çš„å…³é”®å¥é€‰æ‹©åŠŸèƒ½")
    
    print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")


if __name__ == "__main__":
    main()