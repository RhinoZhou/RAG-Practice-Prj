#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¼”ç¤ºæŒ‡æ ‡ä¸ A/B å¯¹æ¯”

ä½œè€…: Ph.D. Rhino
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¥æœŸ: 2024-01-20

åŠŸèƒ½è¯´æ˜:
ç”¨ä¸¤å¥—æ»‘çª—å‚æ•°å¯¹æ¯” P@5ã€Hit@1ã€å†—ä½™ç‡ä¸ p95 å»¶è¿Ÿã€‚

å†…å®¹æ¦‚è¿°:
æ„é€  A/B ä¸¤å¥—"çª—å£/é‡å "é…ç½®ï¼Œè¿è¡Œæœ€å°è¯„ä¼°æµç¨‹ï¼Œè®¡ç®—å¹¶æ‰“å°æ ¸å¿ƒæŒ‡æ ‡ä¸æ¨èç»“è®ºï¼›
è¯¾å ‚å¯å¿«é€Ÿè§‚å¯Ÿå‚æ•°å¯¹å‡†ç¡®æ€§ä¸å†—ä½™/æ—¶å»¶çš„å½±å“ã€‚

ä½¿ç”¨åœºæ™¯:
- RAGç³»ç»Ÿå‚æ•°è°ƒä¼˜æ¼”ç¤º
- è¯¾å ‚æ•™å­¦ä¸­å±•ç¤ºä¸åŒå‚æ•°é…ç½®å¯¹æ£€ç´¢æ€§èƒ½çš„å½±å“
- å¿«é€Ÿè¯„ä¼°æ»‘çª—å‚æ•°å¯¹æ£€ç´¢è´¨é‡å’Œæ•ˆç‡çš„å½±å“

ä¾èµ–åº“:
- numpy: ç”¨äºæ•°å€¼è®¡ç®—å’Œç»Ÿè®¡åˆ†æ
- pandas: ç”¨äºæ•°æ®å¤„ç†å’Œè¡¨æ ¼å±•ç¤º
- scipy: ç”¨äºç»Ÿè®¡å‡½æ•°è®¡ç®—
"""

# è‡ªåŠ¨å®‰è£…ä¾èµ–åº“
import subprocess
import sys
import time
import json
import random

# å®šä¹‰æ‰€éœ€ä¾èµ–åº“
required_dependencies = [
    'numpy',
    'pandas',
    'scipy'
]

def install_dependencies():
    """æ£€æŸ¥å¹¶è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–åº“"""
    for dependency in required_dependencies:
        try:
            # å°è¯•å¯¼å…¥åº“ä»¥æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
            __import__(dependency)
            print(f"âœ… ä¾èµ–åº“ '{dependency}' å·²å®‰è£…")
        except ImportError:
            print(f"âš ï¸ ä¾èµ–åº“ '{dependency}' æœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            # ä½¿ç”¨pipå®‰è£…ç¼ºå¤±çš„ä¾èµ–
            subprocess.check_call([sys.executable, "-m", "pip", "install", dependency])
            print(f"âœ… ä¾èµ–åº“ '{dependency}' å®‰è£…æˆåŠŸ")

# æ‰§è¡Œä¾èµ–å®‰è£…
if __name__ == "__main__":
    install_dependencies()

# å¯¼å…¥æ‰€éœ€åº“
import numpy as np
import pandas as pd
from scipy import stats

class ABEvalMetricsPrinter:
    """A/Bå¯¹æ¯”è¯„ä¼°æŒ‡æ ‡æ‰“å°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–A/Bå¯¹æ¯”è¯„ä¼°æŒ‡æ ‡æ‰“å°å™¨"""
        # å®šä¹‰A/Bä¸¤å¥—æ»‘çª—å‚æ•°é…ç½®
        self.config_a = {
            'name': 'A(win=128,ov=0.3)',
            'window_size': 128,
            'overlap': 0.3
        }
        self.config_b = {
            'name': 'B(win=256,ov=0.5)',
            'window_size': 256,
            'overlap': 0.5
        }
        
        # åˆå§‹åŒ–è¯„ä¼°æ•°æ®
        self.sample_queries = self._load_sample_queries()
        
        # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
        random.seed(42)
        np.random.seed(42)
    
    def _load_sample_queries(self):
        """åŠ è½½ç¤ºä¾‹æŸ¥è¯¢å’Œé‡‘æ ‡å‡†æ•°æ®"""
        # æ¨¡æ‹Ÿçš„æŸ¥è¯¢å’Œé‡‘æ ‡å‡†æ•°æ®
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œå¯ä»¥ä»æ–‡ä»¶æˆ–æ•°æ®åº“ä¸­åŠ è½½çœŸå®çš„æŸ¥è¯¢å’Œé‡‘æ ‡å‡†
        sample_queries = [
            {
                'query': 'æ··åˆæ£€ç´¢çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ',
                'gold_relevant_docs': ['docA_2.2', 'docA_3.1']
            },
            {
                'query': 'æ–‡æœ¬åˆ†å—çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ',
                'gold_relevant_docs': ['docA_3.1', 'docA_3.2']
            },
            {
                'query': 'çª—å£æ»‘åŠ¨ç­–ç•¥çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ',
                'gold_relevant_docs': ['docA_3.2']
            },
            {
                'query': 'TF-IDFçš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ',
                'gold_relevant_docs': ['docA_2.2']
            },
            {
                'query': 'RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿ',
                'gold_relevant_docs': ['docA_2.2', 'docA_3.1', 'docA_3.2']
            }
        ]
        return sample_queries
    
    def _simulate_retrieval_results(self, config):
        """
        æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
        
        å‚æ•°:
            config: æ»‘çª—å‚æ•°é…ç½®
        
        è¿”å›:
            list: æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡
        """
        results = []
        
        for query_info in self.sample_queries:
            query = query_info['query']
            gold_docs = query_info['gold_relevant_docs']
            
            # æ ¹æ®é…ç½®å‚æ•°è°ƒæ•´æ¨¡æ‹Ÿçš„æ£€ç´¢ç»“æœ
            # çª—å£å¤§å°è¶Šå¤§ï¼Œå¬å›ç‡å¯èƒ½è¶Šé«˜ï¼Œä½†å†—ä½™ä¹Ÿå¯èƒ½å¢åŠ 
            # é‡å åº¦è¶Šé«˜ï¼Œå¬å›ç‡å¯èƒ½è¶Šé«˜ï¼Œä½†å†—ä½™å’Œå»¶è¿Ÿä¹Ÿå¯èƒ½å¢åŠ 
            base_precision = 0.5
            base_hit_rate = 0.4
            base_redundancy = 0.2
            base_latency = 80
            
            # æ ¹æ®çª—å£å¤§å°å’Œé‡å åº¦è°ƒæ•´æ€§èƒ½æŒ‡æ ‡
            # çª—å£è¶Šå¤§ï¼Œç²¾åº¦å’Œå¬å›ç‡å¯èƒ½ç•¥é«˜ï¼Œä½†å»¶è¿Ÿå’Œå†—ä½™ä¹Ÿä¼šå¢åŠ 
            precision_factor = 1.0 + (config['window_size'] / 512) * 0.2
            hit_rate_factor = 1.0 + (config['window_size'] / 512) * 0.2
            redundancy_factor = 1.0 + (config['window_size'] / 512) * 0.4 + (config['overlap'] * 0.5)
            latency_factor = 1.0 + (config['window_size'] / 512) * 0.8 + (config['overlap'] * 0.3)
            
            # æ·»åŠ ä¸€äº›éšæœºæ³¢åŠ¨
            precision = base_precision * precision_factor + random.uniform(-0.05, 0.05)
            hit_rate = base_hit_rate * hit_rate_factor + random.uniform(-0.05, 0.05)
            redundancy = base_redundancy * redundancy_factor + random.uniform(-0.05, 0.05)
            latency = base_latency * latency_factor + random.uniform(-10, 10)
            
            # ç¡®ä¿æŒ‡æ ‡åœ¨åˆç†èŒƒå›´å†…
            precision = max(0.1, min(0.9, precision))
            hit_rate = max(0.1, min(0.9, hit_rate))
            redundancy = max(0.05, min(0.5, redundancy))
            latency = max(50, min(200, latency))
            
            # è®°å½•æŸ¥è¯¢çš„ç»“æœ
            results.append({
                'query': query,
                'precision_at_5': precision,
                'hit_at_1': hit_rate,
                'redundancy': redundancy,
                'latency': latency
            })
        
        return results
    
    def _calculate_metrics(self, results):
        """
        è®¡ç®—æ€»ä½“è¯„ä¼°æŒ‡æ ‡
        
        å‚æ•°:
            results: æ£€ç´¢ç»“æœåˆ—è¡¨
        
        è¿”å›:
            dict: æ€»ä½“è¯„ä¼°æŒ‡æ ‡
        """
        # æå–å„ä¸ªæŒ‡æ ‡
        precision_at_5_scores = [r['precision_at_5'] for r in results]
        hit_at_1_scores = [r['hit_at_1'] for r in results]
        redundancy_scores = [r['redundancy'] for r in results]
        latency_scores = [r['latency'] for r in results]
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡å’Œp95å»¶è¿Ÿ
        metrics = {
            'precision_at_5': np.mean(precision_at_5_scores),
            'hit_at_1': np.mean(hit_at_1_scores),
            'redundancy': np.mean(redundancy_scores),
            'p95_latency': stats.scoreatpercentile(latency_scores, 95)
        }
        
        return metrics
    
    def _generate_recommendation(self, metrics_a, metrics_b):
        """
        æ ¹æ®è¯„ä¼°æŒ‡æ ‡ç”Ÿæˆæ¨èæ–¹æ¡ˆ
        
        å‚æ•°:
            metrics_a: æ–¹æ¡ˆAçš„è¯„ä¼°æŒ‡æ ‡
            metrics_b: æ–¹æ¡ˆBçš„è¯„ä¼°æŒ‡æ ‡
        
        è¿”å›:
            str: æ¨èæ–¹æ¡ˆ
        """
        # å®šä¹‰å„æŒ‡æ ‡çš„æƒé‡
        weights = {
            'precision_at_5': 0.3,
            'hit_at_1': 0.3,
            'redundancy': -0.2,  # å†—ä½™ç‡è¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥æƒé‡ä¸ºè´Ÿ
            'p95_latency': -0.2   # å»¶è¿Ÿè¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥æƒé‡ä¸ºè´Ÿ
        }
        
        # è®¡ç®—åŠ æƒå¾—åˆ†
        score_a = (
            metrics_a['precision_at_5'] * weights['precision_at_5'] +
            metrics_a['hit_at_1'] * weights['hit_at_1'] +
            metrics_a['redundancy'] * weights['redundancy'] +
            metrics_a['p95_latency'] * weights['p95_latency'] / 100  # å½’ä¸€åŒ–å»¶è¿Ÿ
        )
        
        score_b = (
            metrics_b['precision_at_5'] * weights['precision_at_5'] +
            metrics_b['hit_at_1'] * weights['hit_at_1'] +
            metrics_b['redundancy'] * weights['redundancy'] +
            metrics_b['p95_latency'] * weights['p95_latency'] / 100  # å½’ä¸€åŒ–å»¶è¿Ÿ
        )
        
        # ç”Ÿæˆæ¨èç»“è®º
        if score_a > score_b:
            recommendation = "æ–¹æ¡ˆ Aï¼ˆå†—ä½™ä½ã€æ—¶å»¶æ›´ä¼˜ï¼‰"
        else:
            recommendation = "æ–¹æ¡ˆ Bï¼ˆç²¾åº¦å’Œå¬å›ç‡æ›´ä¼˜ï¼‰"
        
        return recommendation
    
    def _format_metrics_table(self, metrics_a, metrics_b, recommendation):
        """
        æ ¼å¼åŒ–æŒ‡æ ‡å¯¹æ¯”è¡¨
        
        å‚æ•°:
            metrics_a: æ–¹æ¡ˆAçš„è¯„ä¼°æŒ‡æ ‡
            metrics_b: æ–¹æ¡ˆBçš„è¯„ä¼°æŒ‡æ ‡
            recommendation: æ¨èæ–¹æ¡ˆ
        
        è¿”å›:
            str: æ ¼å¼åŒ–çš„Markdownè¡¨æ ¼
        """
        table = "| æ–¹æ¡ˆ | P@5 | Hit@1 | å†—ä½™ç‡ | p95(ms) |\n"
        table += "|------|-----|-------|--------|---------|\n"
        table += f"| {self.config_a['name']} | {metrics_a['precision_at_5']:.2f} | {metrics_a['hit_at_1']:.2f} | {metrics_a['redundancy']:.2f} | {metrics_a['p95_latency']:.1f} |\n"
        table += f"| {self.config_b['name']} | {metrics_b['precision_at_5']:.2f} | {metrics_b['hit_at_1']:.2f} | {metrics_b['redundancy']:.2f} | {metrics_b['p95_latency']:.1f} |\n"
        table += f"\næ¨èï¼š{recommendation}"
        
        return table
    
    def save_results(self, results_a, results_b, metrics_a, metrics_b, recommendation):
        """
        ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶
        
        å‚æ•°:
            results_a: æ–¹æ¡ˆAçš„è¯¦ç»†ç»“æœ
            results_b: æ–¹æ¡ˆBçš„è¯¦ç»†ç»“æœ
            metrics_a: æ–¹æ¡ˆAçš„è¯„ä¼°æŒ‡æ ‡
            metrics_b: æ–¹æ¡ˆBçš„è¯„ä¼°æŒ‡æ ‡
            recommendation: æ¨èæ–¹æ¡ˆ
        """
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open("ab_eval_detailed_results.json", "w", encoding="utf-8") as f:
            json.dump({
                'config_a': self.config_a,
                'config_b': self.config_b,
                'results_a': results_a,
                'results_b': results_b,
                'metrics_a': metrics_a,
                'metrics_b': metrics_b,
                'recommendation': recommendation
            }, f, ensure_ascii=False, indent=2)
        print("ğŸ“ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: ab_eval_detailed_results.json")
        
        # ä¿å­˜å¯¹æ¯”è¡¨
        table = self._format_metrics_table(metrics_a, metrics_b, recommendation)
        with open("ab_eval_comparison.md", "w", encoding="utf-8") as f:
            f.write(f"# A/B è¯„ä¼°å¯¹æ¯”ç»“æœ\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(table)
        print("ğŸ“ è¯„ä¼°å¯¹æ¯”è¡¨å·²ä¿å­˜è‡³: ab_eval_comparison.md")
    
    def run_evaluation(self):
        """è¿è¡ŒA/Bè¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¯åŠ¨A/Bè¯„ä¼°æŒ‡æ ‡å¯¹æ¯”å·¥å…·")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ¨¡æ‹Ÿæ–¹æ¡ˆAçš„æ£€ç´¢ç»“æœ
        print(f"\nğŸ” è¿è¡Œæ–¹æ¡ˆ {self.config_a['name']} è¯„ä¼°...")
        results_a = self._simulate_retrieval_results(self.config_a)
        metrics_a = self._calculate_metrics(results_a)
        print(f"âœ… æ–¹æ¡ˆ {self.config_a['name']} è¯„ä¼°å®Œæˆ")
        
        # æ¨¡æ‹Ÿæ–¹æ¡ˆBçš„æ£€ç´¢ç»“æœ
        print(f"\nğŸ” è¿è¡Œæ–¹æ¡ˆ {self.config_b['name']} è¯„ä¼°...")
        results_b = self._simulate_retrieval_results(self.config_b)
        metrics_b = self._calculate_metrics(results_b)
        print(f"âœ… æ–¹æ¡ˆ {self.config_b['name']} è¯„ä¼°å®Œæˆ")
        
        # ç”Ÿæˆæ¨è
        recommendation = self._generate_recommendation(metrics_a, metrics_b)
        
        # æ ¼å¼åŒ–å¹¶æ‰“å°å¯¹æ¯”è¡¨
        table = self._format_metrics_table(metrics_a, metrics_b, recommendation)
        print("\nğŸ“Š A/B è¯„ä¼°å¯¹æ¯”ç»“æœ:")
        print(table)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        self.save_results(results_a, results_b, metrics_a, metrics_b, recommendation)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        print(f"\nâ±ï¸  è¯„ä¼°æ‰§è¡Œæ—¶é—´: {end_time - start_time:.4f}ç§’")
        
        # æ£€æŸ¥ä¸­æ–‡è¾“å‡º
        print("\nğŸ” ä¸­æ–‡è¾“å‡ºæµ‹è¯•ï¼šA/Bè¯„ä¼°å¯¹æ¯”å·¥å…·æˆåŠŸå®ç°ä¸åŒæ»‘çª—å‚æ•°é…ç½®çš„æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”")
        
        print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")
        
        return metrics_a, metrics_b, recommendation

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºA/Bè¯„ä¼°å®ä¾‹
    ab_evaluator = ABEvalMetricsPrinter()
    
    # è¿è¡Œè¯„ä¼°
    ab_evaluator.run_evaluation()

if __name__ == "__main__":
    main()